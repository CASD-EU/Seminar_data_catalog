{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#  Ingest metadata of basic entities into open metadata\n",
    "\n",
    "In this tutorial, we will show how to ingest metadata into open metadata.\n",
    "\n",
    "There are many ways to ingest metadat into openmetadata, such as:\n",
    "- connectors\n",
    "- rest API\n",
    "- python SDK\n",
    "\n",
    "In this tutorial, we only how you how to use `python SDK` to ingest metadata.\n",
    "\n",
    "## 1. Set up the python virtual environment\n",
    "\n",
    "Open a conda shell of **python 11** in `Bureau`->`Raccourci`->`Python`. Then enter the below command\n",
    "\n",
    "```shell\n",
    "# 1. Check if conda exists in the current shell\n",
    "conda --version\n",
    "\n",
    "# 2. create a virtual environment\n",
    "conda create --name om-ingestion python --offline\n",
    "# view existing virtual environment list\n",
    "conda env list\n",
    "# check status of a virtual environment\n",
    "conda info --envs\n",
    "\n",
    "# 3. activate a virtual environment\n",
    "conda activate om-ingestion\n",
    "\n",
    "# 4. install packages\n",
    "# check installed package list\n",
    "pip list\n",
    "\n",
    "# install package via requirements.txt\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# 5. verify that you have the required packages\n",
    "pip show pandas\n",
    "pip show openmetadata-ingestion\n",
    "```\n",
    "\n",
    "## 2. Ingest metadata of basic entities\n",
    "\n",
    "The most basic entities in open metadata is the descriptive metadata of data assets. For example\n",
    "- Databases\n",
    "- Tables\n",
    "- Columns\n",
    "- Filesystem\n",
    "- Folder\n",
    "- Files\n",
    "- Etc.\n",
    "\n",
    "In the below example, we will insert the descriptive metadata of Database, Schema, tables, and columns."
   ],
   "id": "7c7b3379c5ec65f7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T13:18:54.513590Z",
     "start_time": "2025-09-09T13:18:54.508630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from metadata.ingestion.ometa.ometa_api import OpenMetadata\n",
    "from metadata.generated.schema.entity.services.connections.metadata.openMetadataConnection import (\n",
    "    OpenMetadataConnection, AuthProvider)\n",
    "from metadata.generated.schema.security.client.openMetadataJWTClientConfig import OpenMetadataJWTClientConfig\n",
    "from metadata.generated.schema.api.services.createStorageService import CreateStorageServiceRequest\n",
    "from metadata.generated.schema.entity.services.storageService import StorageServiceType, StorageConnection\n",
    "from metadata.generated.schema.entity.services.connections.storage.customStorageConnection import \\\n",
    "    CustomStorageConnection, CustomStorageType\n",
    "\n",
    "from metadata.generated.schema.entity.services.storageService import StorageService"
   ],
   "id": "f59f3e090c31d200",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.1 Check open metadata api server connectivity\n",
    "\n",
    "The python-SDK which we use to ingest metadata is an `OM client`, it needs to connect to an `OM server` to ingest metadata.\n",
    "Let's check the connectivity of the server via client."
   ],
   "id": "bc7939334469c9fe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T13:18:32.283527Z",
     "start_time": "2025-09-09T13:18:32.275452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# you need to modify this value to match your target open metadata server url\n",
    "target_om_server = \"http://om-dev.casd.local/api\""
   ],
   "id": "d5ff1a7166ad9e46",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T13:18:34.392483Z",
     "start_time": "2025-09-09T13:18:34.325756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from conf.creds import om_oidc_token\n",
    "\n",
    "server_config = OpenMetadataConnection(\n",
    "    hostPort=target_om_server,\n",
    "    authProvider=AuthProvider.openmetadata,\n",
    "    securityConfig=OpenMetadataJWTClientConfig(\n",
    "        jwtToken=om_oidc_token,\n",
    "    ),\n",
    ")\n",
    "om_con = OpenMetadata(server_config)"
   ],
   "id": "affb21bdd19c3819",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T13:18:35.188065Z",
     "start_time": "2025-09-09T13:18:35.167724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# if it returns true, it means the connection is success \n",
    "om_con.health_check()"
   ],
   "id": "2a8a9344aed2cea3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.2 Ingest metadata of a file system\n",
    "\n",
    "In this section, we will learn how to ingest metadata of a file system into Open metadata server. Suppose we have a file system (i.e. datalake) with the below architecture\n",
    "\n",
    "```text\n",
    "|Constances\n",
    "|   |- geospatial\n",
    "|   |   |- vector\n",
    "|   |   |    |- file1.wkb\n",
    "|   |   |    |- file2.geojson\n",
    "|   |   |- raster\n",
    "|   |   |    |- file3.tif\n",
    "|   |   |    |- file4.nc\n",
    "|   |- clinical\n",
    "|   |   |- file5.parquet\n",
    "|   |   |- file6.csv\n",
    "```\n",
    "To illustrate a file system, open metadata server provides a concept called `StorageService`. A `StorageServer` may contain one or more `Containers`(directories or files).\n",
    "\n",
    "#### 2.2.1 Create a storage service\n",
    "\n",
    "You can consider the storage service as the abstraction of a file system which allows you to store data."
   ],
   "id": "756e265580e196ea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T13:19:03.051216Z",
     "start_time": "2025-09-09T13:19:02.958341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --------------- CONFIGURATION ---------------\n",
    "\n",
    "STORAGE_SERVICE_NAME = \"Constances-Datalake\"\n",
    "STORAGE_SERVICE_DESC = \"Main constances datalake which host all constances related data\"\n",
    "\n",
    "# Step 1: Create the CustomStorageConnection\n",
    "cs_conn = CustomStorageConnection(\n",
    "    type=CustomStorageType.CustomStorage,\n",
    "    connectionOptions={\n",
    "    }\n",
    ")\n",
    "\n",
    "# Step 2: Wrap it inside StorageConnection\n",
    "storage_conn = StorageConnection(config=cs_conn)\n",
    "\n",
    "# Step 3: Create StorageServiceRequest\n",
    "storage_service_request = CreateStorageServiceRequest(\n",
    "    name=STORAGE_SERVICE_NAME,\n",
    "    serviceType=StorageServiceType.CustomStorage,\n",
    "    displayName=STORAGE_SERVICE_NAME,\n",
    "    description=STORAGE_SERVICE_DESC,\n",
    "    connection=storage_conn  # <-- must be StorageConnection, not CustomStorageConnection directly\n",
    ")\n",
    "\n",
    "# Step 4: Create or update in OpenMetadata\n",
    "storage_service_entity = om_con.create_or_update(data=storage_service_request)\n",
    "print(f\"StorageService created: {storage_service_entity}\")"
   ],
   "id": "e4c36bf640770823",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StorageService created: id=Uuid(root=UUID('cc3361c7-85db-4dbd-9ba0-27ce4d91e230')) name=EntityName(root='Constances-Datalake') fullyQualifiedName=FullyQualifiedEntityName(root='Constances-Datalake') displayName='Constances-Datalake' serviceType=<StorageServiceType.CustomStorage: 'CustomStorage'> description=Markdown(root='Main constances datalake which host all constances related data') connection=StorageConnection(config=CustomStorageConnection(type=<CustomStorageType.CustomStorage: 'CustomStorage'>, sourcePythonClass=None, connectionOptions=ConnectionOptions(root={}), containerFilterPattern=None, supportsMetadataExtraction=SupportsMetadataExtraction(root=True))) pipelines=None testConnectionResult=None tags=[] version=EntityVersion(root=0.1) updatedAt=Timestamp(root=1757407607305) updatedBy='ingestion-bot' href=Href(root=AnyUrl('http://localhost:8585/v1/services/storageServices/cc3361c7-85db-4dbd-9ba0-27ce4d91e230')) owners=EntityReferenceList(root=[]) changeDescription=None incrementalChangeDescription=ChangeDescription(fieldsAdded=[], fieldsUpdated=[], fieldsDeleted=[], previousVersion=EntityVersion(root=0.1), changeSummary=None) deleted=False dataProducts=None followers=None domains=None ingestionRunner=None\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 2.2.2 Create containers\n",
    "\n",
    "Open metadata provides a concept called `Container` to represent directories and files. In the below sections we will create\n",
    "containers to represent directory, then we will create containers to represent files\n",
    "\n",
    "We will create the below directories:\n",
    "- geospatial\n",
    "- geospatial/vector\n",
    "- geospatial/raster\n",
    "- clinical"
   ],
   "id": "4a575dcffd5bb24c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T13:19:09.045067Z",
     "start_time": "2025-09-09T13:19:09.035113Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# config containers parameters\n",
    "GEO_PATH = \"/geospatial\"\n",
    "GEO_CONTAINER_NAME = \"geospatial\"\n",
    "GEO_CONTAINER_DESC = \"This folder contains all constances related geospatial data\"\n",
    "\n",
    "VECTOR_PATH = \"/geospatial/vector\"\n",
    "VECTOR_CONTAINER_NAME = \"vector\"\n",
    "VECTOR_CONTAINER_DESC = \"This folder contains all constances related geospatial data in vector format\"\n",
    "\n",
    "RASTER_PATH = \"/geospatial/raster\"\n",
    "RASTER_CONTAINER_NAME = \"raster\"\n",
    "RASTER_CONTAINER_DESC = \"This folder contains all constances related geospatial data in raster format\"\n",
    "\n",
    "CLINICAL_PATH = \"/clinical\"\n",
    "CLINICAL_CONTAINER_NAME = \"clinical\"\n",
    "CLINICAL_CONTAINER_DESC = \"This folder contains all constances related clinical data\""
   ],
   "id": "c4b5d61ce2d67372",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T13:19:09.792117Z",
     "start_time": "2025-09-09T13:19:09.565689Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from metadata.generated.schema.api.data.createContainer import CreateContainerRequest\n",
    "from metadata.generated.schema.entity.data.container import ContainerDataModel\n",
    "from metadata.generated.schema.entity.data.table import Column, DataType\n",
    "from metadata.generated.schema.type.entityReference import EntityReference\n",
    "\n",
    "# -------------------------\n",
    "#  Create geospatial container under storage service\n",
    "# -------------------------\n",
    "geo_dir_request = CreateContainerRequest(\n",
    "    name=GEO_CONTAINER_NAME,\n",
    "    displayName=GEO_CONTAINER_NAME,\n",
    "    description=GEO_CONTAINER_DESC,\n",
    "    service=storage_service_entity.fullyQualifiedName,  # StorageService FQN\n",
    "    fullPath=GEO_PATH,\n",
    ")\n",
    "\n",
    "# Register with OpenMetadata\n",
    "geo_dir_entity = om_con.create_or_update(data=geo_dir_request)\n",
    "print(f\"✅ Container created: {geo_dir_entity.fullyQualifiedName}\")\n",
    "\n",
    "# -------------------------\n",
    "#  Create vector container under geospatial container\n",
    "# -------------------------\n",
    "vector_dir_request = CreateContainerRequest(\n",
    "    name=VECTOR_CONTAINER_NAME,\n",
    "    displayName=VECTOR_CONTAINER_NAME,\n",
    "    description=VECTOR_CONTAINER_DESC,\n",
    "    service=storage_service_entity.fullyQualifiedName,\n",
    "    parent=EntityReference(id=geo_dir_entity.id, type=\"container\"),\n",
    "    fullPath=VECTOR_PATH,\n",
    ")\n",
    "vector_dir_entity = om_con.create_or_update(data=vector_dir_request)\n",
    "print(f\"✅ Container created: {vector_dir_entity.fullyQualifiedName}\")\n",
    "# -------------------------\n",
    "#  Create raster container under geospatial container\n",
    "# -------------------------\n",
    "raster_dir_request = CreateContainerRequest(\n",
    "    name=RASTER_CONTAINER_NAME,\n",
    "    displayName=RASTER_CONTAINER_NAME,\n",
    "    description=RASTER_CONTAINER_DESC,\n",
    "    service=storage_service_entity.fullyQualifiedName,\n",
    "    parent=EntityReference(id=geo_dir_entity.id, type=\"container\"),\n",
    "    fullPath=RASTER_PATH\n",
    ")\n",
    "raster_dir_entity = om_con.create_or_update(data=raster_dir_request)\n",
    "print(f\"✅ Container created: {raster_dir_entity.fullyQualifiedName}\")\n",
    "# -------------------------\n",
    "#  Create clinical container under storage service\n",
    "# -------------------------\n",
    "clinical_dir_request = CreateContainerRequest(\n",
    "    name=CLINICAL_CONTAINER_NAME,\n",
    "    displayName=CLINICAL_CONTAINER_NAME,\n",
    "    description=CLINICAL_CONTAINER_DESC,\n",
    "    service=storage_service_entity.fullyQualifiedName,  # StorageService FQN\n",
    "    fullPath=CLINICAL_PATH,\n",
    ")\n",
    "\n",
    "# Register with OpenMetadata\n",
    "clinical_dir_entity = om_con.create_or_update(data=clinical_dir_request)\n",
    "print(f\"✅ Container created: {clinical_dir_entity.fullyQualifiedName}\")"
   ],
   "id": "c599d5531a4f529b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Container created: root='Constances-Datalake.geospatial'\n",
      "✅ Container created: root='Constances-Datalake.geospatial.vector'\n",
      "✅ Container created: root='Constances-Datalake.geospatial.raster'\n",
      "✅ Container created: root='Constances-Datalake.clinical'\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Create containers to represent files\n",
    "\n",
    "We have created all required directories, now let's create files. Files can also contain schema in open metadata. In the below example, we first create schema(i.e. columns), then we associate these columns to a container(i.e.file)\n"
   ],
   "id": "6d802684fb15ccd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T13:19:13.535167Z",
     "start_time": "2025-09-09T13:19:13.471067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# config file1\n",
    "file1_name = \"file1.wkb\"\n",
    "file1_desc = \"All hospital data in France\"\n",
    "file1_path = f\"{VECTOR_PATH}/{file1_name}\"\n",
    "\n",
    "# define columns of file1\n",
    "file1_columns = [\n",
    "    Column(\n",
    "        name=\"hospital_id\",\n",
    "        displayName=\"Hospital ID\",\n",
    "        dataType=DataType.INT,\n",
    "        description=\"Unique identifier for the hospital\"\n",
    "    ),\n",
    "    Column(\n",
    "        name=\"hospital_name\",\n",
    "        displayName=\"Hospital Name\",\n",
    "        dataType=DataType.STRING,\n",
    "        description=\"Name of the hospital\"\n",
    "    ),\n",
    "    Column(\n",
    "        name=\"location\",\n",
    "        displayName=\"location\",\n",
    "        dataType=DataType.STRING,\n",
    "        description=\"gps coordinates where the hospital is located\"\n",
    "    ),\n",
    "    Column(\n",
    "        name=\"capacity\",\n",
    "        displayName=\"Capacity\",\n",
    "        dataType=DataType.INT,\n",
    "        description=\"Number of beds available\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Build the data model for the container\n",
    "file1_data_model = ContainerDataModel(\n",
    "    isPartitioned=False,\n",
    "    columns=file1_columns\n",
    ")\n",
    "\n",
    "# Create the container request\n",
    "file1_request = CreateContainerRequest(\n",
    "    name=file1_name,\n",
    "    displayName=file1_name,\n",
    "    description=file1_desc,\n",
    "    service=storage_service_entity.fullyQualifiedName,  # must be the FQN of your StorageService\n",
    "    parent=EntityReference(id=vector_dir_entity.id, type=\"container\"),  # must be the parent container FQN\n",
    "    dataModel=file1_data_model,\n",
    "    fullPath=file1_path,\n",
    "    numberOfObjects=1,\n",
    "    size=123.4,\n",
    "    fileFormats=[\"csv\"],\n",
    ")\n",
    "\n",
    "# Register with OpenMetadata\n",
    "file1_entity = om_con.create_or_update(data=file1_request)\n",
    "print(f\"✅ Container created: {file1_entity.fullyQualifiedName}\")"
   ],
   "id": "dcdfd624181db357",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Container created: root='Constances-Datalake.geospatial.vector.\"file1.wkb\"'\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T13:19:14.626738Z",
     "start_time": "2025-09-09T13:19:14.554155Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# config file2\n",
    "file2_name = \"file2.geojson\"\n",
    "file2_desc = \"All patients in constances cohort\"\n",
    "file2_path = f\"{VECTOR_PATH}/{file2_name}\"\n",
    "\n",
    "# define columns of file1\n",
    "file2_columns = [\n",
    "    Column(\n",
    "        name=\"patient_id\",\n",
    "        displayName=\"Patient ID\",\n",
    "        dataType=DataType.INT,\n",
    "        description=\"Unique identifier of patient\"\n",
    "    ),\n",
    "    Column(\n",
    "        name=\"patient_name\",\n",
    "        displayName=\"Patient Name\",\n",
    "        dataType=DataType.STRING,\n",
    "        description=\"Name of the patient\"\n",
    "    ),\n",
    "    Column(\n",
    "        name=\"location\",\n",
    "        displayName=\"location\",\n",
    "        dataType=DataType.STRING,\n",
    "        description=\"gps coordinates where the patient live\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Build the data model for the container\n",
    "file2_data_model = ContainerDataModel(\n",
    "    isPartitioned=False,\n",
    "    columns=file2_columns\n",
    ")\n",
    "\n",
    "# Create the container request\n",
    "file2_request = CreateContainerRequest(\n",
    "    name=file2_name,\n",
    "    displayName=file2_name,\n",
    "    description=file2_desc,\n",
    "    service=storage_service_entity.fullyQualifiedName,  # must be the FQN of your StorageService\n",
    "    parent=EntityReference(id=vector_dir_entity.id, type=\"container\"),  # must be the parent container FQN\n",
    "    dataModel=file2_data_model,\n",
    "    fullPath=file2_path,\n",
    "    numberOfObjects=1,\n",
    "    size=456.7,\n",
    "    fileFormats=[\"json\"],\n",
    ")\n",
    "\n",
    "# Register with OpenMetadata\n",
    "file2_entity = om_con.create_or_update(data=file2_request)\n",
    "print(f\"✅ Container created: {file2_entity.fullyQualifiedName}\")"
   ],
   "id": "ed0a2e643b4b2286",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Container created: root='Constances-Datalake.geospatial.vector.\"file2.geojson\"'\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T13:19:16.761446Z",
     "start_time": "2025-09-09T13:19:16.689569Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# config file3\n",
    "file3_name = \"file3.tif\"\n",
    "file3_desc = \"AVG Temperature of Frace city by month in geotiff format\"\n",
    "file3_path = f\"{RASTER_PATH}/{file3_name}\"\n",
    "\n",
    "# define columns of file1\n",
    "file3_columns = [\n",
    "    Column(\n",
    "        name=\"avg_temperature\",\n",
    "        displayName=\"Average temperature\",\n",
    "        dataType=DataType.INT,\n",
    "        description=\"Average temperature of a pixel in geotiff\"\n",
    "    ),\n",
    "    Column(\n",
    "        name=\"pixel\",\n",
    "        displayName=\"pixel\",\n",
    "        dataType=DataType.STRING,\n",
    "        description=\"Pixel of french city in geotiff\"\n",
    "    ),\n",
    "\n",
    "]\n",
    "\n",
    "# Build the data model for the container\n",
    "file3_data_model = ContainerDataModel(\n",
    "    isPartitioned=False,\n",
    "    columns=file3_columns\n",
    ")\n",
    "\n",
    "# Create the container request\n",
    "file3_request = CreateContainerRequest(\n",
    "    name=file3_name,\n",
    "    displayName=file3_name,\n",
    "    description=file3_desc,\n",
    "    service=storage_service_entity.fullyQualifiedName,  # must be the FQN of your StorageService\n",
    "    parent=EntityReference(id=raster_dir_entity.id, type=\"container\"),  # must be the parent container FQN\n",
    "    dataModel=file3_data_model,\n",
    "    fullPath=file3_path,\n",
    "    numberOfObjects=1,\n",
    "    size=789.7,\n",
    "    fileFormats=[\"csv\"],\n",
    ")\n",
    "\n",
    "# Register with OpenMetadata\n",
    "file3_entity = om_con.create_or_update(data=file3_request)\n",
    "print(f\"✅ Container created: {file3_entity.fullyQualifiedName}\")"
   ],
   "id": "63957bc94ef1e421",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Container created: root='Constances-Datalake.geospatial.raster.\"file3.tif\"'\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T13:19:17.375174Z",
     "start_time": "2025-09-09T13:19:17.313354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# config file4\n",
    "file4_name = \"file4.nc\"\n",
    "file4_desc = \"AVG air pollution of Frace city by month in netcdf format\"\n",
    "file4_path = f\"{RASTER_PATH}/{file4_name}\"\n",
    "\n",
    "# define columns of file4\n",
    "file4_columns = [\n",
    "    Column(\n",
    "        name=\"avg_air_pollution\",\n",
    "        displayName=\"Average air pollution\",\n",
    "        dataType=DataType.INT,\n",
    "        description=\"Average air pollution of a pixel in netcdf\"\n",
    "    ),\n",
    "    Column(\n",
    "        name=\"pixel\",\n",
    "        displayName=\"pixel\",\n",
    "        dataType=DataType.STRING,\n",
    "        description=\"Pixel of french city in netcdf\"\n",
    "    ),\n",
    "\n",
    "]\n",
    "\n",
    "# Build the data model for the container\n",
    "file4_data_model = ContainerDataModel(\n",
    "    isPartitioned=False,\n",
    "    columns=file4_columns\n",
    ")\n",
    "\n",
    "# Create the container request\n",
    "file4_request = CreateContainerRequest(\n",
    "    name=file4_name,\n",
    "    displayName=file4_name,\n",
    "    description=file4_desc,\n",
    "    service=storage_service_entity.fullyQualifiedName,  # must be the FQN of your StorageService\n",
    "    parent=EntityReference(id=raster_dir_entity.id, type=\"container\"),  # must be the parent container FQN\n",
    "    dataModel=file4_data_model,\n",
    "    fullPath=file4_path,\n",
    "    numberOfObjects=1,\n",
    "    size=666.7,\n",
    "    fileFormats=[\"csv\"],\n",
    ")\n",
    "\n",
    "# Register with OpenMetadata\n",
    "file4_entity = om_con.create_or_update(data=file4_request)\n",
    "print(f\"✅ Container created: {file4_entity.fullyQualifiedName}\")"
   ],
   "id": "74f0250c33f9841f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Container created: root='Constances-Datalake.geospatial.raster.\"file4.nc\"'\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T13:19:17.916501Z",
     "start_time": "2025-09-09T13:19:17.908837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_file_entity(om_server_con, file_name: str, file_desc: str, file_path: str, file_columns, storage_service,\n",
    "                       parent_dir, file_size: float):\n",
    "    \"\"\"\n",
    "    This function will create a file entity(container) under a directory entity(container) inside a storage service.\n",
    "    :param om_server_con: open metadata server connection\n",
    "    :param file_name:\n",
    "    :param file_desc:\n",
    "    :param file_path:\n",
    "    :param file_columns: A list of columns\n",
    "    :param storage_service:\n",
    "    :param parent_dir:\n",
    "    :param file_size:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Build the data model for the container\n",
    "    file_data_model = ContainerDataModel(\n",
    "        isPartitioned=False,\n",
    "        columns=file_columns\n",
    "    )\n",
    "\n",
    "    # Create the container request\n",
    "    file_request = CreateContainerRequest(\n",
    "        name=file_name,\n",
    "        displayName=file_name,\n",
    "        description=file_desc,\n",
    "        service=storage_service.fullyQualifiedName,  # must be the FQN of your StorageService\n",
    "        parent=EntityReference(id=parent_dir.id, type=\"container\"),  # must be the parent container FQN\n",
    "        dataModel=file_data_model,\n",
    "        fullPath=file_path,\n",
    "        numberOfObjects=1,\n",
    "        size=file_size,\n",
    "        fileFormats=[\"csv\"],\n",
    "    )\n",
    "\n",
    "    # Register with OpenMetadata\n",
    "    file_entity = om_server_con.create_or_update(data=file_request)\n",
    "    print(f\"✅ Container created: {file_entity.fullyQualifiedName}\")"
   ],
   "id": "8ac40730f5b16a38",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T13:19:22.249886Z",
     "start_time": "2025-09-09T13:19:22.180245Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# config file5\n",
    "file5_name = \"file5.parquet\"\n",
    "file5_desc = \"blood test of a patient\"\n",
    "file5_path = f\"{RASTER_PATH}/{file5_name}\"\n",
    "file5_size = 345.6\n",
    "# define columns of file5\n",
    "file5_columns = [\n",
    "    Column(\n",
    "        name=\"blood_test_id\",\n",
    "        displayName=\"blood test id\",\n",
    "        dataType=DataType.INT,\n",
    "        description=\"Unique identifier of the blood test\"\n",
    "    ),\n",
    "    Column(\n",
    "        name=\"patient_name\",\n",
    "        displayName=\"patient Name\",\n",
    "        dataType=DataType.STRING,\n",
    "        description=\"Name of the patient\"\n",
    "    ),\n",
    "    Column(\n",
    "        name=\"red_cell_count\",\n",
    "        displayName=\"red cell number count\",\n",
    "        dataType=DataType.INT,\n",
    "        description=\"Number of red cells\"\n",
    "    ),\n",
    "\n",
    "]\n",
    "\n",
    "create_file_entity(om_con, file5_name, file5_desc, file5_path, file5_columns, storage_service_entity,\n",
    "                   clinical_dir_entity, file5_size)"
   ],
   "id": "ee0a53052653ec81",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Container created: root='Constances-Datalake.clinical.\"file5.parquet\"'\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T13:19:24.111876Z",
     "start_time": "2025-09-09T13:19:24.048756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# config file6\n",
    "file6_name = \"file6.csv\"\n",
    "file6_desc = \"general test of a patient\"\n",
    "file6_path = f\"{RASTER_PATH}/{file6_name}\"\n",
    "file6_size = 8888.8\n",
    "# define columns of file6\n",
    "file6_columns = [\n",
    "    Column(\n",
    "        name=\"general_test_id\",\n",
    "        displayName=\"general test id\",\n",
    "        dataType=DataType.INT,\n",
    "        description=\"Unique identifier of the general test\"\n",
    "    ),\n",
    "    Column(\n",
    "        name=\"patient_name\",\n",
    "        displayName=\"patient Name\",\n",
    "        dataType=DataType.STRING,\n",
    "        description=\"Name of the patient\"\n",
    "    ),\n",
    "    Column(\n",
    "        name=\"patient_weight\",\n",
    "        displayName=\"patient weight\",\n",
    "        dataType=DataType.INT,\n",
    "        description=\"weight of a patient\"\n",
    "    ),\n",
    "\n",
    "    Column(\n",
    "        name=\"patient_height\",\n",
    "        displayName=\"patient height\",\n",
    "        dataType=DataType.INT,\n",
    "        description=\"height of a patient\"\n",
    "    ),\n",
    "\n",
    "]\n",
    "\n",
    "create_file_entity(om_con, file6_name, file6_desc, file6_path, file6_columns, storage_service_entity,\n",
    "                   clinical_dir_entity, file6_size)"
   ],
   "id": "851e0ec182030012",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Container created: root='Constances-Datalake.clinical.\"file6.csv\"'\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.3 Ingest the metadata of a database\n",
    "\n",
    "We have seen how to ingest metadata of a file system, now lets see how to ingest metadata of a database. Suppose we have a mysql database called `hospitals_in_france`. We want to ingest metadata of this database into OM. So other users can use this database.\n",
    "\n",
    "To ingest metdata of a database, the architecture of the database must be respected as below:\n",
    "`DatabaseService`-> `Database`->`Schema`->"
   ],
   "id": "916b5af61a02ce9a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T13:34:21.757902Z",
     "start_time": "2025-09-09T13:34:21.659202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from metadata.generated.schema.api.services.createDatabaseService import CreateDatabaseServiceRequest\n",
    "from metadata.generated.schema.entity.services.connections.database.common.basicAuth import BasicAuth\n",
    "from metadata.generated.schema.entity.services.connections.database.mysqlConnection import MysqlConnection\n",
    "from metadata.generated.schema.entity.services.databaseService import (DatabaseConnection, DatabaseService,\n",
    "                                                                       DatabaseServiceType, )\n",
    "\n",
    "# name of the db service\n",
    "DB_SERVICE_NAME = \"Constances-Geography\"\n",
    "# description of the service\n",
    "DB_SERVICE_DESC = \"This database service stores all geography databases of INSERM\"\n",
    "\n",
    "DB_AUTH_LOGIN = \"db_login\"\n",
    "DB_AUTH_PWD = \"db_pwd\"\n",
    "DB_URL = \"http://db_url:1234\"\n",
    "\n",
    "db_service = CreateDatabaseServiceRequest(\n",
    "    name=DB_SERVICE_NAME,\n",
    "    serviceType=DatabaseServiceType.Mysql,\n",
    "    connection=DatabaseConnection(\n",
    "        config=MysqlConnection(\n",
    "            username=DB_AUTH_LOGIN,\n",
    "            authType=BasicAuth(password=DB_AUTH_PWD),\n",
    "            hostPort=DB_URL,\n",
    "        )\n",
    "    ),\n",
    "    description=DB_SERVICE_DESC,\n",
    ")\n",
    "\n",
    "# when we create an entity by using function `create_or_update`, it returns the created instance of the query\n",
    "db_service_entity = om_con.create_or_update(data=db_service)"
   ],
   "id": "2e37614cb861d481",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# you can view the content of the returned object to check if your request is executed correctly.\n",
    "print(db_service_entity)"
   ],
   "id": "c6edef46d810807a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from metadata.generated.schema.api.data.createDatabase import CreateDatabaseRequest\n",
    "\n",
    "DB_NAME = \"hospitals_in_france\"\n",
    "\n",
    "db_entity_req = CreateDatabaseRequest(\n",
    "    name=DB_NAME,\n",
    "    service=db_service_entity.fullyQualifiedName,\n",
    "    description=\"In this database, we store all tables which contain geographical information in Constances\",\n",
    ")\n",
    "\n",
    "db_entity = om_con.create_or_update(data=db_entity_req)"
   ],
   "id": "70856d7e4a869e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from metadata.generated.schema.api.data.createDatabaseSchema import CreateDatabaseSchemaRequest\n",
    "\n",
    "SCHEMA_NAME = \"Geography\"\n",
    "create_schema_req = CreateDatabaseSchemaRequest(\n",
    "    name=SCHEMA_NAME,\n",
    "    database=db_entity.fullyQualifiedName,\n",
    "    description=\"In this schema, we group all tables which contain geographical information of hospitals in France\", )\n",
    "\n",
    "# the create request will return the fqn(fully qualified name) of the created schema\n",
    "schema_entity = om_con.create_or_update(data=create_schema_req)"
   ],
   "id": "3b0d61d6d7f53636",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step2: Get metadata from source files\n",
    "\n",
    "Here we use two files to describe metadata:\n",
    "- <project_name>_tables: describes the metadata of tables in this project\n",
    "- <project_name_vars>: describes the metadata of the columns in this project "
   ],
   "id": "70b622ba52b61c33"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pathlib\n",
    "\n",
    "project_root = pathlib.Path.cwd().parent\n",
    "metadata_path = project_root / \"data\"\n",
    "\n",
    "print(metadata_path)"
   ],
   "id": "d8d4f57e7e7921c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "table_spec_path = f\"{metadata_path}/constances_tables.csv\"\n",
    "col_spec_path = f\"{metadata_path}/constances_vars.csv\"\n",
    "\n"
   ],
   "id": "69330e6c60b62cd7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "table_df = pd.read_csv(table_spec_path, header=0)\n",
    "print(table_df.head(5))"
   ],
   "id": "6711b2784ba2c72e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "col_df = pd.read_csv(col_spec_path, header=0)\n",
    "print(col_df.head(5))"
   ],
   "id": "1dea237874908789",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from metadata.generated.schema.api.data.createTable import CreateTableRequest\n",
    "from metadata.generated.schema.entity.data.table import Column, DataType\n",
    "\n",
    "\n",
    "def getColDetailsByTabName(table_name: str, col_df):\n",
    "    # filter the rows that belongs to the given table name\n",
    "    table_col_list = col_df[col_df[\"table\"] == table_name].to_dict(orient=\"records\")\n",
    "    return table_col_list\n",
    "\n",
    "\n",
    "target_tab_name = \"fr_communes_raw\"\n",
    "tab_col_list = getColDetailsByTabName(target_tab_name, col_df)\n",
    "\n",
    "for item in tab_col_list:\n",
    "    print(f\"table name: {item['table']}\")\n",
    "    print(f\"column name: {item['var']}\")\n",
    "    print(f\"column type: {item['var_type']}\")\n",
    "    print(f\"column size: {item['var_size']}\")\n",
    "    print(f\"column description: {item['description']}\")"
   ],
   "id": "9fdaa20baae10c5d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 3. clean the metadata before ingestion  \n",
    "\n",
    "We need to clean the raw metadata before ingestion, because the value may not be compatible with `Open metadata`.\n",
    "For example, the column types in `Open metadata` are pre-defined. Only the valid value can be inserted into the `Open metadata` server. "
   ],
   "id": "9eb1a594613849ea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from metadata.generated.schema.entity.data.table import Column, DataType\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "# util func\n",
    "authorized_str_type = [\"string\", \"str\", ]\n",
    "authorized_int_type = [\"int\", \"integer\"]\n",
    "authorized_long_type = [\"bigint\", \"long\"]\n",
    "\n",
    "\n",
    "def get_om_dtype(in_type: str) -> DataType:\n",
    "    # test input type is not null and is string\n",
    "    if in_type and isinstance(in_type, str):\n",
    "        # cast it to lower case to ignor case\n",
    "        in_type_val = in_type.lower()\n",
    "        # we create a mapping case for all sql types\n",
    "        if in_type_val == \"tinyint\":\n",
    "            return DataType.TINYINT\n",
    "        elif in_type_val == \"byte\":\n",
    "            return DataType.BYTEINT\n",
    "        elif in_type_val == \"smallint\":\n",
    "            return DataType.SMALLINT\n",
    "        elif in_type_val in authorized_int_type:\n",
    "            return DataType.INT\n",
    "        elif in_type_val in authorized_long_type:\n",
    "            return DataType.BIGINT\n",
    "        elif in_type_val == 'numeric':\n",
    "            return DataType.NUMERIC\n",
    "        elif in_type_val == 'number':\n",
    "            return DataType.NUMBER\n",
    "        elif in_type_val == 'float':\n",
    "            return DataType.FLOAT\n",
    "        elif in_type_val == 'double':\n",
    "            return DataType.DOUBLE\n",
    "        elif in_type_val == 'date':\n",
    "            return DataType.DATE\n",
    "        elif in_type_val == 'time':\n",
    "            return DataType.TIME\n",
    "        elif in_type_val == \"char\":\n",
    "            return DataType.CHAR\n",
    "        elif in_type_val == \"varchar\":\n",
    "            return DataType.VARCHAR\n",
    "        elif in_type_val == \"text\":\n",
    "            return DataType.TEXT\n",
    "        elif in_type_val == \"ntext\":\n",
    "            return DataType.NTEXT\n",
    "        elif in_type_val == \"binary\":\n",
    "            return DataType.BINARY\n",
    "        elif in_type_val == \"varbinary\":\n",
    "            return DataType.VARBINARY\n",
    "        # other types\n",
    "        elif in_type_val in authorized_str_type:\n",
    "            return DataType.STRING\n",
    "        # for complex map such as array<int>, map<int,string>\n",
    "        # we must use dataTypeDisplay to show the details. In dataType, we can only put array, map\n",
    "        elif in_type_val == \"array\":\n",
    "            return DataType.ARRAY\n",
    "        elif in_type_val == \"map\":\n",
    "            return DataType.MAP\n",
    "        elif in_type_val == \"struct\":\n",
    "            return DataType.STRUCT\n",
    "        # for geometry type\n",
    "        elif in_type_val == \"geometry\":\n",
    "            return DataType.GEOMETRY\n",
    "        # for empty string, we use string as default value\n",
    "        elif in_type_val == \"\":\n",
    "            return DataType.STRING\n",
    "\n",
    "        else:\n",
    "            return DataType.UNKNOWN\n",
    "    else:\n",
    "        print(f\"The input value {in_type} is not a valid string type\")\n",
    "        raise ValueError\n",
    "\n",
    "\n",
    "def build_type_display_name(type_val: str, length: Optional[int], precision: Optional[int]) -> str:\n",
    "    \"\"\"\n",
    "    This function build a data type display value, it only considers three case, because the result return by \n",
    "    split_length_precision only has three possible case\n",
    "    :param type_val: data type value (e.g. string, int, etc.) \n",
    "    :type type_val: str\n",
    "    :param length: full length of the type \n",
    "    :type length: Optional[int]\n",
    "    :param precision: precision of the type \n",
    "    :type precision: Optional[int]\n",
    "    :return: data type display value\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    if length and precision:\n",
    "        return f\"{type_val}({length},{precision})\"\n",
    "    elif length and not precision:\n",
    "        return f\"{type_val}({length})\"\n",
    "    else:\n",
    "        return type_val\n",
    "\n",
    "\n",
    "def split_length_precision(raw_type_size: str) -> (int, int):\n",
    "    \"\"\"\n",
    "    This function parse the raw type size (e.g. 3 or 5,3) into a tuple of (length, precision).\n",
    "    Some example\n",
    "     - 3 to (3,None)\n",
    "     - 5,3 to (5,3).\n",
    "     - None or not string to (None,None)\n",
    "     - \"\" to (None,None)\n",
    "     - ,3 to (None,None) because it does not make sense if only return precision\n",
    "    :param raw_type_size:\n",
    "    :type raw_type_size:\n",
    "    :return:\n",
    "    :rtype:\n",
    "    \"\"\"\n",
    "    length = None\n",
    "    precision = None\n",
    "    # if it's null or not string, return none,none\n",
    "    if raw_type_size and isinstance(raw_type_size, str):\n",
    "        # if the size is not empty string, do split\n",
    "        if len(raw_type_size) > 0:\n",
    "            split_res = raw_type_size.split(\",\", 1)\n",
    "            # if it has two items after split, it has length and precision\n",
    "            try:\n",
    "                if len(split_res) == 2:\n",
    "                    length = int(split_res[0])\n",
    "                    precision = int(split_res[1])\n",
    "                else:\n",
    "                    length = int(split_res[0])\n",
    "            except ValueError as e:\n",
    "                print(f\"The length:{split_res[0]} or precision{split_res[1]} can't be cast to int.\")\n",
    "\n",
    "    return length, precision\n",
    "\n",
    "\n",
    "def generate_om_column_entity(col_details: List[Dict]) -> List[Column]:\n",
    "    \"\"\"\n",
    "    This functions takes the column details of a tables, it generates a list of openmetadata column entity\n",
    "    :param col_details: \n",
    "    :type col_details: \n",
    "    :return: \n",
    "    :rtype: \n",
    "    \"\"\"\n",
    "    columns: List[Column] = []\n",
    "    for col_detail in col_details:\n",
    "        col_name = col_detail['var']\n",
    "        type_val = col_detail['var_type'].lower()\n",
    "        type_size = col_detail['var_size']\n",
    "        length, precision = split_length_precision(type_size)\n",
    "        data_type = get_om_dtype(type_val)\n",
    "        type_display_val = build_type_display_name(type_val, length, precision)\n",
    "        col_desc = col_detail['description']\n",
    "        # for array data type, we must also provide the datatype inside the array, here we set string for simplicity\n",
    "        if data_type == DataType.ARRAY:\n",
    "            array_data_type = DataType.STRING\n",
    "        else:\n",
    "            array_data_type = None\n",
    "        # for struct data type,\n",
    "        if data_type == DataType.STRUCT:\n",
    "            children = [{\"version\": DataType.INT}, {\"timestamp\": DataType.TIME}]\n",
    "        else:\n",
    "            children = None\n",
    "        col_entity = Column(name=col_name, dataType=data_type, arrayDataType=array_data_type, children=children,\n",
    "                            dataTypeDisplay=type_display_val, dataLength=length, precision=precision,\n",
    "                            description=col_desc)\n",
    "        columns.append(col_entity)\n",
    "    return columns"
   ],
   "id": "977b30bed9bf7f6c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## Load metadata of all tables\n",
    "from metadata.generated.schema.api.data.createTable import CreateTableRequest\n",
    "\n",
    "# step1: loop the table list to get table name and description\n",
    "table_list = table_df[['table', 'description']].to_dict(orient=\"records\")\n",
    "\n",
    "for tab in table_list:\n",
    "    tab_name = tab['table']\n",
    "    tab_desc = tab['description']\n",
    "    print(f\"tab_name:{tab_name}, tab_desc:{tab_desc}\")\n",
    "    # step2: get tab col list\n",
    "    tab_col_list = getColDetailsByTabName(tab_name, col_df)\n",
    "    # step3: loop through the col list and build the OM colum list\n",
    "    columns = generate_om_column_entity(tab_col_list)\n",
    "    # step4: create table\n",
    "    table_create = CreateTableRequest(\n",
    "        name=tab_name,\n",
    "        description=tab_desc,\n",
    "        databaseSchema=schema_entity.fullyQualifiedName,\n",
    "        columns=columns)\n",
    "    table_entity = om_con.create_or_update(data=table_create)"
   ],
   "id": "6ac38aa593e6a7e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.4 Ingest metadata of a external storage server\n",
    "\n",
    "We have seen how to ingest metadata of file system and database. If the storage server is hosted at public cloud such as Amazon and GCP. It's also possible to ingest the metadata into the open metadata server. In the below section, we will ingest the metadata of a S3 storage server in AWS."
   ],
   "id": "b91dbc0787b53296"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T13:21:18.292333Z",
     "start_time": "2025-09-09T13:21:18.194434Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from metadata.generated.schema.security.credentials.awsCredentials import AWSCredentials\n",
    "from metadata.generated.schema.entity.services.connections.storage.s3Connection import S3Connection\n",
    "\n",
    "# 1. Create S3 Storage Service\n",
    "s3_conn = S3Connection(\n",
    "    awsConfig=AWSCredentials(\n",
    "        awsAccessKeyId=\"YOUR_ACCESS_KEY\",\n",
    "        awsSecretAccessKey=\"YOUR_SECRET_KEY\",\n",
    "        awsRegion=\"us-east-1\",\n",
    "        assumeRoleArn=None\n",
    "    ),\n",
    "    bucketNames=[\"Constance\"]  # must be a list\n",
    ")\n",
    "\n",
    "s3_service_req = CreateStorageServiceRequest(\n",
    "    name=\"Constance-AWS\",\n",
    "    serviceType=StorageServiceType.S3,\n",
    "    connection=StorageConnection(config=s3_conn),\n",
    "    description=\"Constances AWS S3 data lake\"\n",
    ")\n",
    "s3_service_entity = om_con.create_or_update(data=s3_service_req)\n",
    "\n",
    "# 2. Create a container for development\n",
    "dev_dir_name = \"development\"\n",
    "dev_dir_req = CreateContainerRequest(\n",
    "    name=dev_dir_name,\n",
    "    displayName=dev_dir_name,\n",
    "    service=s3_service_entity.fullyQualifiedName,\n",
    ")\n",
    "\n",
    "dev_dir_entity = om_con.create_or_update(data=dev_dir_req)\n",
    "print(dev_dir_entity)"
   ],
   "id": "21f828af3762c2aa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id=Uuid(root=UUID('c108cb6f-8123-4fea-8c58-b52ec2bd8c67')) name=EntityName(root='development') fullyQualifiedName=FullyQualifiedEntityName(root='Constance-AWS.development') displayName='development' description=None version=EntityVersion(root=0.1) updatedAt=Timestamp(root=1757423980780) updatedBy='ingestion-bot' href=Href(root=AnyUrl('http://localhost:8585/v1/containers/c108cb6f-8123-4fea-8c58-b52ec2bd8c67')) owners=EntityReferenceList(root=[]) service=EntityReference(id=Uuid(root=UUID('241cb5ff-4706-4766-810a-b00725c9e08e')), type='storageService', name='Constance-AWS', fullyQualifiedName='Constance-AWS', description=Markdown(root='Constances AWS S3 data lake'), displayName='Constance-AWS', deleted=False, inherited=None, href=Href(root=AnyUrl('http://localhost:8585/v1/services/storageServices/241cb5ff-4706-4766-810a-b00725c9e08e'))) parent=None children=None dataModel=None prefix=None numberOfObjects=None size=None fileFormats=None serviceType=<StorageServiceType.S3: 'S3'> followers=None tags=[] changeDescription=None incrementalChangeDescription=ChangeDescription(fieldsAdded=[], fieldsUpdated=[], fieldsDeleted=[], previousVersion=EntityVersion(root=0.1), changeSummary=None) deleted=False retentionPeriod=None extension=None sourceUrl=None fullPath=None domains=EntityReferenceList(root=[]) dataProducts=None votes=None lifeCycle=None certification=None sourceHash=None\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T13:22:48.627745Z",
     "start_time": "2025-09-09T13:22:48.471077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# create a data model\n",
    "prs_columns = [\n",
    "    Column(\n",
    "        name=\"patient_id\",\n",
    "        displayName=\"Patient ID\",\n",
    "        dataType=DataType.INT,\n",
    "        description=\"Unique identifier of patient\"\n",
    "    ),\n",
    "    Column(\n",
    "        name=\"patient_name\",\n",
    "        displayName=\"Patient Name\",\n",
    "        dataType=DataType.STRING,\n",
    "        description=\"Name of the patient\"\n",
    "    ),\n",
    "    Column(\n",
    "        name=\"location\",\n",
    "        displayName=\"location\",\n",
    "        dataType=DataType.STRING,\n",
    "        description=\"gps coordinates where the patient live\"\n",
    "    ),\n",
    "    Column(\n",
    "        name=\"total_sum\",\n",
    "        displayName=\"total_sum\",\n",
    "        dataType=DataType.FLOAT,\n",
    "        description=\"Total payment of the social security\"\n",
    "    ),\n",
    "\n",
    "]\n",
    "\n",
    "# Build the data model for the container\n",
    "prs_data_model = ContainerDataModel(\n",
    "    isPartitioned=False,\n",
    "    columns=prs_columns\n",
    ")\n",
    "\n",
    "# create a container, it must belong to a service. Here we use a storage service\n",
    "container_req = CreateContainerRequest(name='prs_2015_03_08',\n",
    "                                       displayName='prs_2015_03_08',\n",
    "                                       description='this parquet dataset contains the prs data',\n",
    "                                       parent=EntityReference(id=dev_dir_entity.id, type=\"container\"),\n",
    "                                       service=s3_service_entity.fullyQualifiedName,\n",
    "                                       dataModel=prs_data_model,\n",
    "                                       numberOfObjects=3,\n",
    "                                       size=123456.75,\n",
    "                                       fileFormats=['parquet', ]\n",
    "                                       , )\n",
    "\n",
    "container_entity = om_con.create_or_update(data=container_req)\n"
   ],
   "id": "e64ac7c3c57373ab",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Clean up\n",
    "\n",
    "We have created many metadata entities, if we want to clean them, we can call the below function.\n"
   ],
   "id": "ee2443979be2930f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T13:38:59.592613Z",
     "start_time": "2025-09-09T13:38:59.586024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def delete_storage_service(storage_service_name:str):\n",
    "    \"\"\"\n",
    "    This function takes a database service name, if existed, it will remove the database service and all\n",
    "    metadata entities under the database services. If not, a warning message will be shown.\n",
    "    :param storage_service_name:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # get database service id\n",
    "    try:\n",
    "        # try to get the db service\n",
    "        service_id = om_con.get_by_name(\n",
    "            entity=StorageService, fqn=storage_service_name\n",
    "        ).id\n",
    "        print(f\"Find the storage service with id: {service_id}\")\n",
    "        print(f\"Start the delete process\")\n",
    "        # delete the service by using id\n",
    "        om_con.delete(\n",
    "            entity=StorageService,\n",
    "            entity_id=service_id,\n",
    "            recursive=True,\n",
    "            hard_delete=True,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Cant find a storage service with the given name {storage_service_name}: {e}\")\n",
    "        return"
   ],
   "id": "ff703f7603fabc12",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T13:40:53.676538Z",
     "start_time": "2025-09-09T13:40:43.535603Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# delete the custom storage service\n",
    "\n",
    "STORE_SER_NAME = \"Constances-Datalake\"\n",
    "delete_storage_service(STORE_SER_NAME)"
   ],
   "id": "20549119fa56f219",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find the storage service with id: root=UUID('cc3361c7-85db-4dbd-9ba0-27ce4d91e230')\n",
      "Start the delete process\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def delete_db_service(db_service_name:str):\n",
    "    \"\"\"\n",
    "    This function takes a database service name, if existed, it will remove the database service and all\n",
    "    metadata entities under the database services. If not a warning message will be shown.\n",
    "    :param db_service_name:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # get database service id\n",
    "    try:\n",
    "        # try to get the db service\n",
    "        service_id = om_con.get_by_name(\n",
    "            entity=DatabaseService, fqn=db_service_name\n",
    "        ).id\n",
    "        print(f\"Find the database service with id: {service_id}\")\n",
    "        print(f\"Start the delete process\")\n",
    "        # delete the service by using id\n",
    "        om_con.delete(\n",
    "            entity=DatabaseService,\n",
    "            entity_id=service_id,\n",
    "            recursive=True,\n",
    "            hard_delete=True,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Cant find a database service with the given name {db_service_name}: {e}\")\n",
    "        return\n"
   ],
   "id": "8ab394b7a8dd2a96",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "DB_SERVICE_NAME = \"\"\n",
    "delete_db_service(DB_SERVICE_NAME)"
   ],
   "id": "2f686ad3b1131b4c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T13:45:06.730440Z",
     "start_time": "2025-09-09T13:45:05.246157Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# delete the S3 storage\n",
    "s3_storage_name = \"Constance_datalake\"\n",
    "delete_storage_service(s3_storage_name)"
   ],
   "id": "41e749a8f77ecd6e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find the storage service with id: root=UUID('754f4ae3-e3b8-4d10-94c4-99b1c57c6461')\n",
      "Start the delete process\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7663efea7624359f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
