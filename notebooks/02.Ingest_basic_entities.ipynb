{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#  Ingest metadata of basic entities into open metadata\n",
    "\n",
    "In this tutorial, we will show how to ingest metadata into open metadata.\n",
    "\n",
    "There are many ways to ingest metadat into openmetadata, such as:\n",
    "- connectors\n",
    "- rest API\n",
    "- python SDK\n",
    "\n",
    "In this tutorial, we only how you how to use `python SDK` to ingest metadata.\n",
    "\n",
    "## 1. Set up the python virtual environment\n",
    "\n",
    "Open a conda shell of **python 11** in `Bureau`->`Raccourci`->`Python`. Then enter the below command\n",
    "\n",
    "```shell\n",
    "# 1. Check if conda exists in the current shell\n",
    "conda --version\n",
    "\n",
    "# 2. create a virtual environment\n",
    "conda create --name om-ingestion python --offline\n",
    "# view existing virtual environment list\n",
    "conda env list\n",
    "# check status of a virtual environment\n",
    "conda info --envs\n",
    "\n",
    "# 3. activate a virtual environment\n",
    "conda activate om-ingestion\n",
    "\n",
    "# 4. install packages\n",
    "# check installed package list\n",
    "pip list\n",
    "\n",
    "# install package via requirements.txt\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# 5. verify that you have the required packages\n",
    "pip show pandas\n",
    "pip show openmetadata-ingestion\n",
    "```\n",
    "\n",
    "## 2. Ingest metadata of basic entities\n",
    "\n",
    "The most basic entities in open metadata is the descriptive metadata of data assets. For example\n",
    "- Databases\n",
    "- Tables\n",
    "- Columns\n",
    "- Filesystem\n",
    "- Folder\n",
    "- Files\n",
    "- Etc.\n",
    "\n",
    "In the below example, we will insert the descriptive metadata of Database, Schema, tables, and columns."
   ],
   "id": "7c7b3379c5ec65f7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T07:34:22.819178Z",
     "start_time": "2025-09-09T07:34:11.373665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from metadata.ingestion.ometa.ometa_api import OpenMetadata\n",
    "from metadata.generated.schema.entity.services.connections.metadata.openMetadataConnection import (OpenMetadataConnection, AuthProvider)\n",
    "from metadata.generated.schema.security.client.openMetadataJWTClientConfig import OpenMetadataJWTClientConfig"
   ],
   "id": "f59f3e090c31d200",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.1 Check open metadata api server connectivity\n",
    "\n",
    "The python-SDK which we use to ingest metadata is an `OM client`, it needs to connect to an `OM server` to ingest metadata.\n",
    "Let's check the connectivity of the server via client."
   ],
   "id": "bc7939334469c9fe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T07:34:22.829301Z",
     "start_time": "2025-09-09T07:34:22.822488Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# you need to modify this value to match your target open metadata server url\n",
    "target_om_server=\"http://om-dev.casd.local/api\""
   ],
   "id": "d5ff1a7166ad9e46",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T07:34:22.891601Z",
     "start_time": "2025-09-09T07:34:22.837394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from conf.creds import om_oidc_token\n",
    "server_config = OpenMetadataConnection(\n",
    "    hostPort=target_om_server,\n",
    "    authProvider=AuthProvider.openmetadata,\n",
    "    securityConfig=OpenMetadataJWTClientConfig(\n",
    "        jwtToken=om_oidc_token,\n",
    "    ),\n",
    ")\n",
    "om_con = OpenMetadata(server_config)"
   ],
   "id": "affb21bdd19c3819",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T07:34:22.916406Z",
     "start_time": "2025-09-09T07:34:22.901120Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# if it returns true, it means the connection is success \n",
    "om_con.health_check()"
   ],
   "id": "2a8a9344aed2cea3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.2 Ingest the metadata of a database\n",
    "\n",
    "Suppose we have a mysql database called `hospitals_in_france`. We want to ingest metadata of this database into OM. So other users can use this database."
   ],
   "id": "916b5af61a02ce9a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T13:39:03.828206Z",
     "start_time": "2025-09-08T13:39:03.749050Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from metadata.generated.schema.api.services.createDatabaseService import CreateDatabaseServiceRequest\n",
    "from metadata.generated.schema.entity.services.connections.database.common.basicAuth import BasicAuth\n",
    "from metadata.generated.schema.entity.services.connections.database.mysqlConnection import MysqlConnection\n",
    "from metadata.generated.schema.entity.services.databaseService import (DatabaseConnection, DatabaseService, DatabaseServiceType,)\n",
    "\n",
    "# name of the db service\n",
    "DB_SERVICE_NAME = \"Constances-Geography\"\n",
    "# description of the service\n",
    "DB_SERVICE_DESC = \"This database service stores all geography databases of INSERM\"\n",
    "\n",
    "DB_AUTH_LOGIN = \"db_login\"\n",
    "DB_AUTH_PWD = \"db_pwd\"\n",
    "DB_URL = \"http://db_url:1234\"\n",
    "\n",
    "db_service = CreateDatabaseServiceRequest(\n",
    "    name=DB_SERVICE_NAME,\n",
    "    serviceType=DatabaseServiceType.Mysql,\n",
    "    connection=DatabaseConnection(\n",
    "        config=MysqlConnection(\n",
    "            username=DB_AUTH_LOGIN,\n",
    "            authType=BasicAuth(password=DB_AUTH_PWD),\n",
    "            hostPort=DB_URL,\n",
    "        )\n",
    "    ),\n",
    "    description=DB_SERVICE_DESC,\n",
    ")\n",
    "\n",
    "# when we create an entity by using function `create_or_update`, it returns the created instance of the query\n",
    "db_service_entity = om_con.create_or_update(data=db_service)"
   ],
   "id": "2e37614cb861d481",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T13:39:05.213138Z",
     "start_time": "2025-09-08T13:39:05.206059Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# you can view the content of the returned object to check if your request is executed correctly.\n",
    "print(db_service_entity)"
   ],
   "id": "c6edef46d810807a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id=Uuid(root=UUID('c726e413-a5de-41d0-a9b7-25fb11197d6b')) name=EntityName(root='Constances-Geography') fullyQualifiedName=FullyQualifiedEntityName(root='Constances-Geography') displayName=None serviceType=<DatabaseServiceType.Mysql: 'Mysql'> description=Markdown(root='This database service stores all geography databases of INSERM') connection=DatabaseConnection(config=MysqlConnection(type=<MySQLType.Mysql: 'Mysql'>, scheme=<MySQLScheme.mysql_pymysql: 'mysql+pymysql'>, username='db_login', authType=BasicAuth(password=SecretStr('**********')), hostPort='http://db_url:1234', databaseName=None, databaseSchema=None, sslConfig=None, connectionOptions=None, connectionArguments=None, schemaFilterPattern=FilterPattern(includes=[], excludes=['^information_schema$', '^performance_schema$']), tableFilterPattern=None, databaseFilterPattern=None, supportsMetadataExtraction=SupportsMetadataExtraction(root=True), supportsDBTExtraction=SupportsDBTExtraction(root=True), supportsProfiler=SupportsProfiler(root=True), supportsQueryComment=SupportsQueryComment(root=True), sampleDataStorageConfig=None, supportsDataDiff=SupportsDataDiff(root=True), supportsUsageExtraction=SupportsUsageExtraction(root=True), supportsLineageExtraction=SupportsLineageExtraction(root=True), useSlowLogs=False)) pipelines=None testConnectionResult=None tags=[] version=EntityVersion(root=0.2) updatedAt=Timestamp(root=1757338743781) updatedBy='ingestion-bot' owners=EntityReferenceList(root=[]) href=Href(root=AnyUrl('http://localhost:8585/v1/services/databaseServices/c726e413-a5de-41d0-a9b7-25fb11197d6b')) followers=None changeDescription=ChangeDescription(fieldsAdded=[], fieldsUpdated=[FieldChange(name=FieldName(root='connection'), oldValue='\"old-encrypted-value\"', newValue='\"new-encrypted-value\"')], fieldsDeleted=[], previousVersion=EntityVersion(root=0.1), changeSummary=ChangeSummaryModel(root={})) incrementalChangeDescription=ChangeDescription(fieldsAdded=[], fieldsUpdated=[FieldChange(name=FieldName(root='connection'), oldValue='\"old-encrypted-value\"', newValue='\"new-encrypted-value\"')], fieldsDeleted=[], previousVersion=EntityVersion(root=0.1), changeSummary=ChangeSummaryModel(root={})) deleted=False dataProducts=None domains=None ingestionRunner=None\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T13:39:07.484609Z",
     "start_time": "2025-09-08T13:39:07.424351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from metadata.generated.schema.api.data.createDatabase import CreateDatabaseRequest\n",
    "DB_NAME = \"hospitals_in_france\"\n",
    "\n",
    "db_entity_req = CreateDatabaseRequest(\n",
    "    name=DB_NAME,\n",
    "    service=db_service_entity.fullyQualifiedName,\n",
    "    description=\"In this database, we store all tables which contain geographical information in Constances\",\n",
    ")\n",
    "\n",
    "db_entity = om_con.create_or_update(data=db_entity_req)"
   ],
   "id": "70856d7e4a869e6",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T13:39:10.167559Z",
     "start_time": "2025-09-08T13:39:10.087311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from metadata.generated.schema.api.data.createDatabaseSchema import CreateDatabaseSchemaRequest\n",
    "SCHEMA_NAME = \"Geography\"\n",
    "create_schema_req = CreateDatabaseSchemaRequest(\n",
    "    name=SCHEMA_NAME, \n",
    "    database=db_entity.fullyQualifiedName,\n",
    "    description=\"In this schema, we group all tables which contain geographical information of hospitals in France\",)\n",
    "\n",
    "# the create request will return the fqn(fully qualified name) of the created schema\n",
    "schema_entity = om_con.create_or_update(data=create_schema_req)"
   ],
   "id": "3b0d61d6d7f53636",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step2: Get metadata from source files\n",
    "\n",
    "Here we use two files to describe metadata:\n",
    "- <project_name>_tables: describes the metadata of tables in this project\n",
    "- <project_name_vars>: describes the metadata of the columns in this project "
   ],
   "id": "70b622ba52b61c33"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T13:39:20.982473Z",
     "start_time": "2025-09-08T13:39:20.975116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pathlib\n",
    "project_root = pathlib.Path.cwd().parent\n",
    "metadata_path = project_root / \"data\"\n",
    "\n",
    "print(metadata_path)"
   ],
   "id": "d8d4f57e7e7921c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PLIU\\Documents\\git\\Seminare_data_catalog\\data\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T13:39:21.576875Z",
     "start_time": "2025-09-08T13:39:21.573298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "table_spec_path = f\"{metadata_path}/constances_tables.csv\"\n",
    "col_spec_path = f\"{metadata_path}/constances_vars.csv\"\n",
    "\n"
   ],
   "id": "69330e6c60b62cd7",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T13:39:22.423182Z",
     "start_time": "2025-09-08T13:39:22.374018Z"
    }
   },
   "cell_type": "code",
   "source": [
    "table_df = pd.read_csv(table_spec_path,header=0)\n",
    "print(table_df.head(5))"
   ],
   "id": "6711b2784ba2c72e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       domain                  table  \\\n",
      "0       INSEE        fr_communes_raw   \n",
      "1  Constances      fr_communes_clean   \n",
      "2         OSM         osm_france_raw   \n",
      "3  Constances    osm_hospitals_clean   \n",
      "4  Constances  hospitals_in_communes   \n",
      "\n",
      "                                         description  creation  suppression  \n",
      "0  This table contains all geographical informati...      2022          NaN  \n",
      "1  This table is built based on fr_communes_raw w...      2024          NaN  \n",
      "2  This table is the open street map of france. I...      2020          NaN  \n",
      "3  This table is build based on osm_france_raw. I...      2024          NaN  \n",
      "4  This table contains the number of hospitals in...      2024          NaN  \n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T13:39:23.302186Z",
     "start_time": "2025-09-08T13:39:23.274126Z"
    }
   },
   "cell_type": "code",
   "source": [
    "col_df = pd.read_csv(col_spec_path,header=0)\n",
    "print(col_df.head(5))"
   ],
   "id": "1dea237874908789",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             table        var                                    description  \\\n",
      "0  fr_communes_raw   geometry  geo location of the commune in a polygon form   \n",
      "1  fr_communes_raw  wikipedia            url of the wiki page of the commune   \n",
      "2  fr_communes_raw    surf_ha          number of habitats inside the commune   \n",
      "3  fr_communes_raw        nom                            name of the commune   \n",
      "4  fr_communes_raw      insee                      code insee of the commune   \n",
      "\n",
      "   var_type var_size    nomencalure  creation  suppression  \n",
      "0  geometry       18  geometry_type      2024          NaN  \n",
      "1    string       28            NaN      2024          NaN  \n",
      "2    number        8            NaN      2024          NaN  \n",
      "3    string       26            NaN      2024          NaN  \n",
      "4    string        5     code_insee      2024          NaN  \n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T13:39:24.273687Z",
     "start_time": "2025-09-08T13:39:24.260020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from metadata.generated.schema.api.data.createTable import CreateTableRequest\n",
    "from metadata.generated.schema.entity.data.table import Column, DataType\n",
    "\n",
    "def getColDetailsByTabName(table_name:str, col_df):\n",
    "    # filter the rows that belongs to the given table name\n",
    "    table_col_list=col_df[col_df[\"table\"]==table_name].to_dict(orient=\"records\")\n",
    "    return table_col_list\n",
    "    \n",
    "target_tab_name = \"fr_communes_raw\"\n",
    "tab_col_list=getColDetailsByTabName(target_tab_name, col_df)\n",
    "\n",
    "for item in tab_col_list:\n",
    "    print(f\"table name: {item['table']}\")\n",
    "    print(f\"column name: {item['var']}\")\n",
    "    print(f\"column type: {item['var_type']}\")\n",
    "    print(f\"column size: {item['var_size']}\")\n",
    "    print(f\"column description: {item['description']}\")"
   ],
   "id": "9fdaa20baae10c5d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table name: fr_communes_raw\n",
      "column name: geometry\n",
      "column type: geometry\n",
      "column size: 18\n",
      "column description: geo location of the commune in a polygon form\n",
      "table name: fr_communes_raw\n",
      "column name: wikipedia\n",
      "column type: string\n",
      "column size: 28\n",
      "column description: url of the wiki page of the commune\n",
      "table name: fr_communes_raw\n",
      "column name: surf_ha\n",
      "column type: number\n",
      "column size: 8\n",
      "column description: number of habitats inside the commune\n",
      "table name: fr_communes_raw\n",
      "column name: nom\n",
      "column type: string\n",
      "column size: 26\n",
      "column description: name of the commune\n",
      "table name: fr_communes_raw\n",
      "column name: insee\n",
      "column type: string\n",
      "column size: 5\n",
      "column description: code insee of the commune\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 3. clean the metadata before ingestion  \n",
    "\n",
    "We need to clean the raw metadata before ingestion, because the value may not be compatible with `Open metadata`.\n",
    "For example, the column types in `Open metadata` are pre-defined. Only the valid value can be inserted into the `Open metadata` server. "
   ],
   "id": "9eb1a594613849ea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T13:39:31.790004Z",
     "start_time": "2025-09-08T13:39:31.774586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from metadata.generated.schema.entity.data.table import Column, DataType\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "# util func\n",
    "authorized_str_type=[\"string\",\"str\",]\n",
    "authorized_int_type=[\"int\",\"integer\"]\n",
    "authorized_long_type=[\"bigint\",\"long\"]\n",
    "\n",
    "def get_om_dtype(in_type:str)->DataType:\n",
    "    # test input type is not null and is string\n",
    "    if in_type and isinstance(in_type, str):\n",
    "        # cast it to lower case to ignor case\n",
    "        in_type_val=in_type.lower()\n",
    "        # we create a mapping case for all sql types\n",
    "        if in_type_val == \"tinyint\":\n",
    "            return DataType.TINYINT\n",
    "        elif in_type_val == \"byte\":\n",
    "            return DataType.BYTEINT\n",
    "        elif in_type_val == \"smallint\":\n",
    "            return DataType.SMALLINT\n",
    "        elif in_type_val in authorized_int_type:\n",
    "            return DataType.INT\n",
    "        elif in_type_val in authorized_long_type:\n",
    "            return DataType.BIGINT\n",
    "        elif in_type_val=='numeric':\n",
    "            return DataType.NUMERIC\n",
    "        elif in_type_val=='number':\n",
    "            return DataType.NUMBER\n",
    "        elif in_type_val=='float':\n",
    "            return DataType.FLOAT\n",
    "        elif in_type_val=='double':\n",
    "            return DataType.DOUBLE\n",
    "        elif in_type_val=='date':\n",
    "            return DataType.DATE\n",
    "        elif in_type_val=='time':\n",
    "            return DataType.TIME\n",
    "        elif in_type_val==\"char\":\n",
    "            return DataType.CHAR\n",
    "        elif in_type_val==\"varchar\":\n",
    "            return DataType.VARCHAR\n",
    "        elif in_type_val==\"text\":\n",
    "            return DataType.TEXT\n",
    "        elif in_type_val==\"ntext\":\n",
    "            return DataType.NTEXT\n",
    "        elif in_type_val==\"binary\":\n",
    "            return DataType.BINARY\n",
    "        elif in_type_val==\"varbinary\":\n",
    "            return DataType.VARBINARY\n",
    "        # other types\n",
    "        elif in_type_val in authorized_str_type:\n",
    "            return DataType.STRING\n",
    "        # for complex map such as array<int>, map<int,string>\n",
    "        # we must use dataTypeDisplay to show the details. In dataType, we can only put array, map\n",
    "        elif in_type_val==\"array\":\n",
    "            return DataType.ARRAY\n",
    "        elif in_type_val==\"map\":\n",
    "            return DataType.MAP\n",
    "        elif in_type_val==\"struct\":\n",
    "            return DataType.STRUCT\n",
    "        # for geometry type\n",
    "        elif in_type_val==\"geometry\":\n",
    "            return DataType.GEOMETRY\n",
    "        # for empty string, we use string as default value\n",
    "        elif in_type_val==\"\":\n",
    "            return DataType.STRING\n",
    "        \n",
    "        else:\n",
    "            return DataType.UNKNOWN\n",
    "    else:\n",
    "        print(f\"The input value {in_type} is not a valid string type\")\n",
    "        raise ValueError\n",
    "    \n",
    "\n",
    "def build_type_display_name(type_val: str, length: Optional[int], precision: Optional[int]) -> str:\n",
    "    \"\"\"\n",
    "    This function build a data type display value, it only considers three case, because the result return by \n",
    "    split_length_precision only has three possible case\n",
    "    :param type_val: data type value (e.g. string, int, etc.) \n",
    "    :type type_val: str\n",
    "    :param length: full length of the type \n",
    "    :type length: Optional[int]\n",
    "    :param precision: precision of the type \n",
    "    :type precision: Optional[int]\n",
    "    :return: data type display value\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    if length and precision:\n",
    "        return f\"{type_val}({length},{precision})\"\n",
    "    elif length and not precision:\n",
    "        return f\"{type_val}({length})\"\n",
    "    else:\n",
    "        return type_val\n",
    "\n",
    "def split_length_precision(raw_type_size: str) -> (int, int):\n",
    "    \"\"\"\n",
    "    This function parse the raw type size (e.g. 3 or 5,3) into a tuple of (length, precision).\n",
    "    Some example\n",
    "     - 3 to (3,None)\n",
    "     - 5,3 to (5,3).\n",
    "     - None or not string to (None,None)\n",
    "     - \"\" to (None,None)\n",
    "     - ,3 to (None,None) because it does not make sense if only return precision\n",
    "    :param raw_type_size:\n",
    "    :type raw_type_size:\n",
    "    :return:\n",
    "    :rtype:\n",
    "    \"\"\"\n",
    "    length = None\n",
    "    precision = None\n",
    "    # if it's null or not string, return none,none\n",
    "    if raw_type_size and isinstance(raw_type_size, str):\n",
    "        # if the size is not empty string, do split\n",
    "        if len(raw_type_size) > 0:\n",
    "            split_res = raw_type_size.split(\",\", 1)\n",
    "            # if it has two items after split, it has length and precision\n",
    "            try:\n",
    "                if len(split_res) == 2:\n",
    "                    length = int(split_res[0])\n",
    "                    precision = int(split_res[1])\n",
    "                else:\n",
    "                    length = int(split_res[0])\n",
    "            except ValueError as e:\n",
    "                print(f\"The length:{split_res[0]} or precision{split_res[1]} can't be cast to int.\")\n",
    "\n",
    "    return length, precision\n",
    "    \n",
    "def generate_om_column_entity(col_details:List[Dict])->List[Column]:\n",
    "    \"\"\"\n",
    "    This functions takes the column details of a tables, it generates a list of openmetadata column entity\n",
    "    :param col_details: \n",
    "    :type col_details: \n",
    "    :return: \n",
    "    :rtype: \n",
    "    \"\"\"\n",
    "    columns:List[Column]=[]\n",
    "    for col_detail in col_details:\n",
    "        col_name=col_detail['var']\n",
    "        type_val=col_detail['var_type'].lower()\n",
    "        type_size=col_detail['var_size']\n",
    "        length, precision=split_length_precision(type_size)\n",
    "        data_type=get_om_dtype(type_val)\n",
    "        type_display_val=build_type_display_name(type_val,length,precision)\n",
    "        col_desc=col_detail['description']\n",
    "        # for array data type, we must also provide the datatype inside the array, here we set string for simplicity\n",
    "        if data_type==DataType.ARRAY:\n",
    "            array_data_type=DataType.STRING\n",
    "        else:\n",
    "            array_data_type=None\n",
    "        # for struct data type,\n",
    "        if data_type==DataType.STRUCT:\n",
    "            children=[{\"version\":DataType.INT},{\"timestamp\":DataType.TIME}]\n",
    "        else:\n",
    "            children=None\n",
    "        col_entity=Column(name=col_name, dataType=data_type, arrayDataType=array_data_type, children=children, dataTypeDisplay=type_display_val, dataLength=length,precision=precision,description=col_desc)\n",
    "        columns.append(col_entity)\n",
    "    return columns"
   ],
   "id": "977b30bed9bf7f6c",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T13:39:32.924670Z",
     "start_time": "2025-09-08T13:39:32.504571Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Load metadata of all tables\n",
    "from metadata.generated.schema.api.data.createTable import CreateTableRequest\n",
    "# step1: loop the table list to get table name and description\n",
    "table_list=table_df[['table','description']].to_dict(orient=\"records\")\n",
    "\n",
    "for tab in table_list:\n",
    "    tab_name=tab['table']\n",
    "    tab_desc=tab['description']\n",
    "    print(f\"tab_name:{tab_name}, tab_desc:{tab_desc}\")\n",
    "    # step2: get tab col list\n",
    "    tab_col_list=getColDetailsByTabName(tab_name, col_df)\n",
    "    # step3: loop through the col list and build the OM colum list\n",
    "    columns = generate_om_column_entity(tab_col_list)\n",
    "    # step4: create table\n",
    "    table_create=CreateTableRequest(\n",
    "    name=tab_name,\n",
    "    description=tab_desc,\n",
    "    databaseSchema=schema_entity.fullyQualifiedName,\n",
    "    columns=columns)\n",
    "    table_entity=om_con.create_or_update(data=table_create)"
   ],
   "id": "6ac38aa593e6a7e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tab_name:fr_communes_raw, tab_desc:This table contains all geographical information of french communes\n",
      "tab_name:fr_communes_clean, tab_desc:This table is built based on fr_communes_raw which is suitable for Contances related analysis\n",
      "tab_name:osm_france_raw, tab_desc:This table is the open street map of france. It contains all geographical information such as roads hospitals in france\n",
      "tab_name:osm_hospitals_clean, tab_desc:This table is build based on osm_france_raw. It only contains geographical information of hospitals in france\n",
      "tab_name:hospitals_in_communes, tab_desc:This table contains the number of hospitals in each communes\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Ingest metadata of a file system\n",
    "\n",
    "We have seen how to ingest metadata of a database, now lets see how to ingest metadata of a file system. Suppose we have a datalake with the below architecture\n",
    "\n",
    "```text\n",
    "|Constances\n",
    "|   |- geospatial\n",
    "|   |   |- vector\n",
    "|   |   |    |- file1.wkb\n",
    "|   |   |    |- file2.geojson\n",
    "|   |   |- raster\n",
    "|   |   |    |- file3.tif\n",
    "|   |   |    |- file4.nc\n",
    "|   |- clinical\n",
    "|   |   |- file5.parquet\n",
    "|   |   |- file6.csv\n",
    "```\n"
   ],
   "id": "76c9cb168808d5bf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Create a storage service\n",
    "\n",
    "You can consider the storage service as the abstraction of a file system which allows you to store data."
   ],
   "id": "769e0c3a2ffc6827"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T08:46:47.355725Z",
     "start_time": "2025-09-09T08:46:47.237572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from metadata.generated.schema.api.services.createStorageService import CreateStorageServiceRequest\n",
    "from metadata.generated.schema.entity.services.storageService import StorageServiceType, StorageConnection\n",
    "from metadata.generated.schema.entity.services.connections.storage.customStorageConnection import \\\n",
    "    CustomStorageConnection, CustomStorageType\n",
    "\n",
    "from metadata.generated.schema.entity.services.storageService import StorageService\n",
    "\n",
    "# --------------- CONFIGURATION ---------------\n",
    "\n",
    "STORAGE_SERVICE_NAME = \"Constances-Datalake\"\n",
    "STORAGE_SERVICE_DESC = \"Main constances datalake which host all constances related data\"\n",
    "\n",
    "\n",
    "# Step 1: Create the CustomStorageConnection\n",
    "cs_conn = CustomStorageConnection(\n",
    "    type=CustomStorageType.CustomStorage,\n",
    "    connectionOptions={\n",
    "    }\n",
    ")\n",
    "\n",
    "# Step 2: Wrap it inside StorageConnection\n",
    "storage_conn = StorageConnection(config=cs_conn)\n",
    "\n",
    "# Step 3: Create StorageServiceRequest\n",
    "storage_service_request = CreateStorageServiceRequest(\n",
    "    name=STORAGE_SERVICE_NAME,\n",
    "    serviceType=StorageServiceType.CustomStorage,\n",
    "    displayName=STORAGE_SERVICE_NAME,\n",
    "    description=STORAGE_SERVICE_DESC,\n",
    "    connection=storage_conn  # <-- must be StorageConnection, not CustomStorageConnection directly\n",
    ")\n",
    "\n",
    "# Step 4: Create or update in OpenMetadata\n",
    "storage_service_entity = om_con.create_or_update(data=storage_service_request)\n",
    "print(f\"StorageService created: {storage_service_entity}\")"
   ],
   "id": "1de29494a25d92ec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StorageService created: id=Uuid(root=UUID('cc3361c7-85db-4dbd-9ba0-27ce4d91e230')) name=EntityName(root='Constances-Datalake') fullyQualifiedName=FullyQualifiedEntityName(root='Constances-Datalake') displayName='Constances-Datalake' serviceType=<StorageServiceType.CustomStorage: 'CustomStorage'> description=Markdown(root='Main constances datalake which host all constances related data') connection=StorageConnection(config=CustomStorageConnection(type=<CustomStorageType.CustomStorage: 'CustomStorage'>, sourcePythonClass=None, connectionOptions=ConnectionOptions(root={}), containerFilterPattern=None, supportsMetadataExtraction=SupportsMetadataExtraction(root=True))) pipelines=None testConnectionResult=None tags=[] version=EntityVersion(root=0.1) updatedAt=Timestamp(root=1757407607305) updatedBy='ingestion-bot' href=Href(root=AnyUrl('http://localhost:8585/v1/services/storageServices/cc3361c7-85db-4dbd-9ba0-27ce4d91e230')) owners=None changeDescription=None incrementalChangeDescription=None deleted=False dataProducts=None followers=None domains=None ingestionRunner=None\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Create directory\n",
    "\n",
    "We need to create two directory:\n",
    "- geospatial\n",
    "- clinical"
   ],
   "id": "f25dec8b0e6dd3d3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T09:11:09.242806Z",
     "start_time": "2025-09-09T09:11:09.235146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "GEO_PATH = \"/geospatial\"\n",
    "GEO_CONTAINER_NAME = \"geospatial\"\n",
    "GEO_CONTAINER_DESC = \"This folder contains all constances related geospatial data\"\n",
    "\n",
    "VECTOR_PATH = \"/geospatial/vector\"\n",
    "VECTOR_CONTAINER_NAME = \"vector\"\n",
    "VECTOR_CONTAINER_DESC = \"This folder contains all constances related geospatial data in vector format\"\n",
    "\n",
    "RASTER_PATH = \"/geospatial/raster\"\n",
    "RASTER_CONTAINER_NAME = \"raster\"\n",
    "RASTER_CONTAINER_DESC = \"This folder contains all constances related geospatial data in raster format\"\n",
    "\n",
    "CLINICAL_PATH = \"/clinical\"\n",
    "CLINICAL_CONTAINER_NAME = \"clinical\"\n",
    "CLINICAL_CONTAINER_DESC = \"This folder contains all constances related clinical data\""
   ],
   "id": "4a8d13a7a0c5f063",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T09:12:16.375839Z",
     "start_time": "2025-09-09T09:12:16.136203Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from metadata.generated.schema.type import lifeCycle\n",
    "from metadata.generated.schema.api.data.createContainer import CreateContainerRequest\n",
    "from metadata.generated.schema.entity.data.container import ContainerDataModel\n",
    "from metadata.generated.schema.entity.data.table import Column, DataType\n",
    "from metadata.generated.schema.type.entityReference import EntityReference\n",
    "\n",
    "# -------------------------\n",
    "#  Create geospatial container under storage service\n",
    "# -------------------------\n",
    "geo_dir_request = CreateContainerRequest(\n",
    "    name=GEO_CONTAINER_NAME,\n",
    "    displayName=GEO_CONTAINER_NAME,\n",
    "    description=GEO_CONTAINER_DESC,\n",
    "    service=storage_service_entity.fullyQualifiedName,  # StorageService FQN\n",
    "    fullPath=GEO_PATH,\n",
    ")\n",
    "\n",
    "# Register with OpenMetadata\n",
    "geo_dir_entity = om_con.create_or_update(data=geo_dir_request)\n",
    "print(f\"✅ Container created: {geo_dir_entity.fullyQualifiedName}\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "#  Create vector container under geospatial container\n",
    "# -------------------------\n",
    "vector_dir_request = CreateContainerRequest(\n",
    "    name=VECTOR_CONTAINER_NAME,\n",
    "    displayName=VECTOR_CONTAINER_NAME,\n",
    "    description=VECTOR_CONTAINER_DESC,\n",
    "    service=storage_service_entity.fullyQualifiedName,\n",
    "    parent=EntityReference(id=geo_dir_entity.id, type=\"container\"),\n",
    "    fullPath=VECTOR_PATH,\n",
    ")\n",
    "vector_dir_entity = om_con.create_or_update(data=vector_dir_request)\n",
    "print(f\"✅ Container created: {vector_dir_entity.fullyQualifiedName}\")\n",
    "# -------------------------\n",
    "#  Create raster container under geospatial container\n",
    "# -------------------------\n",
    "raster_dir_request = CreateContainerRequest(\n",
    "    name=RASTER_CONTAINER_NAME,\n",
    "    displayName=RASTER_CONTAINER_NAME,\n",
    "    description=RASTER_CONTAINER_DESC,\n",
    "    service=storage_service_entity.fullyQualifiedName,\n",
    "    parent=EntityReference(id=geo_dir_entity.id, type=\"container\"),\n",
    "    fullPath=RASTER_PATH\n",
    ")\n",
    "raster_dir_entity = om_con.create_or_update(data=raster_dir_request)\n",
    "print(f\"✅ Container created: {raster_dir_entity.fullyQualifiedName}\")\n",
    "# -------------------------\n",
    "#  Create clinical container under storage service\n",
    "# -------------------------\n",
    "clinical_dir_request = CreateContainerRequest(\n",
    "    name=CLINICAL_CONTAINER_NAME,\n",
    "    displayName=CLINICAL_CONTAINER_NAME,\n",
    "    description=CLINICAL_CONTAINER_DESC,\n",
    "    service=storage_service_entity.fullyQualifiedName,  # StorageService FQN\n",
    "    fullPath=CLINICAL_PATH,\n",
    ")\n",
    "\n",
    "# Register with OpenMetadata\n",
    "clinical_dir_entity = om_con.create_or_update(data=clinical_dir_request)\n",
    "print(f\"✅ Container created: {clinical_dir_entity.fullyQualifiedName}\")"
   ],
   "id": "e92a6875ae91165a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Container created: root='Constances-Datalake.geospatial'\n",
      "✅ Container created: root='Constances-Datalake.geospatial.vector'\n",
      "✅ Container created: root='Constances-Datalake.geospatial.raster'\n",
      "✅ Container created: root='Constances-Datalake.clinical'\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Create files\n",
    "\n",
    "We have created directories, now let's create files. Files can also contain schema. In the below example, we first create columns, then we associate these columns to a file"
   ],
   "id": "b91dbc0787b53296"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T09:25:29.939618Z",
     "start_time": "2025-09-09T09:25:29.781351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# config file1\n",
    "file1_name = \"file1.wkb\"\n",
    "file1_desc = \"All hospital data in France\"\n",
    "file1_path = f\"{VECTOR_PATH}/{file1_name}\"\n",
    "\n",
    "# define columns of file1\n",
    "file1_columns = [\n",
    "    Column(\n",
    "        name=\"hospital_id\",\n",
    "        displayName=\"Hospital ID\",\n",
    "        dataType=DataType.INT,\n",
    "        description=\"Unique identifier for the hospital\"\n",
    "    ),\n",
    "    Column(\n",
    "        name=\"hospital_name\",\n",
    "        displayName=\"Hospital Name\",\n",
    "        dataType=DataType.STRING,\n",
    "        description=\"Name of the hospital\"\n",
    "    ),\n",
    "    Column(\n",
    "        name=\"location\",\n",
    "        displayName=\"location\",\n",
    "        dataType=DataType.STRING,\n",
    "        description=\"gps coordinates where the hospital is located\"\n",
    "    ),\n",
    "    Column(\n",
    "        name=\"capacity\",\n",
    "        displayName=\"Capacity\",\n",
    "        dataType=DataType.INT,\n",
    "        description=\"Number of beds available\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Build the data model for the container\n",
    "file1_data_model = ContainerDataModel(\n",
    "    isPartitioned=False,\n",
    "    columns=file1_columns\n",
    ")\n",
    "\n",
    "# Create the container request\n",
    "file1_request = CreateContainerRequest(\n",
    "    name=file1_name,\n",
    "    displayName=file1_name,\n",
    "    description=file1_desc,\n",
    "    service=storage_service_entity.fullyQualifiedName,   # must be the FQN of your StorageService\n",
    "    parent=EntityReference(id=vector_dir_entity.id, type=\"container\"), # must be the parent container FQN\n",
    "    dataModel=file1_data_model,\n",
    "    fullPath=file1_path,\n",
    "    numberOfObjects=1,\n",
    "    size=123.4,\n",
    "    fileFormats=[\"csv\"],\n",
    ")\n",
    "\n",
    "# Register with OpenMetadata\n",
    "file1_entity = om_con.create_or_update(data=file1_request)\n",
    "print(f\"✅ Container created: {file1_entity.fullyQualifiedName}\")"
   ],
   "id": "a18fb81dda0b8ceb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Container created: root='Constances-Datalake.geospatial.vector.\"file1.wkb\"'\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T10:03:13.773437Z",
     "start_time": "2025-09-09T10:03:13.590650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# config file2\n",
    "file2_name = \"file2.geojson\"\n",
    "file2_desc = \"All patients in constances cohort\"\n",
    "file2_path = f\"{VECTOR_PATH}/{file2_name}\"\n",
    "\n",
    "# define columns of file1\n",
    "file2_columns = [\n",
    "    Column(\n",
    "        name=\"patient_id\",\n",
    "        displayName=\"Patient ID\",\n",
    "        dataType=DataType.INT,\n",
    "        description=\"Unique identifier of patient\"\n",
    "    ),\n",
    "    Column(\n",
    "        name=\"patient_name\",\n",
    "        displayName=\"Patient Name\",\n",
    "        dataType=DataType.STRING,\n",
    "        description=\"Name of the patient\"\n",
    "    ),\n",
    "    Column(\n",
    "        name=\"location\",\n",
    "        displayName=\"location\",\n",
    "        dataType=DataType.STRING,\n",
    "        description=\"gps coordinates where the patient live\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Build the data model for the container\n",
    "file2_data_model = ContainerDataModel(\n",
    "    isPartitioned=False,\n",
    "    columns=file2_columns\n",
    ")\n",
    "\n",
    "# Create the container request\n",
    "file2_request = CreateContainerRequest(\n",
    "    name=file2_name,\n",
    "    displayName=file2_name,\n",
    "    description=file2_desc,\n",
    "    service=storage_service_entity.fullyQualifiedName,   # must be the FQN of your StorageService\n",
    "    parent=EntityReference(id=vector_dir_entity.id, type=\"container\"), # must be the parent container FQN\n",
    "    dataModel=file2_data_model,\n",
    "    fullPath=file2_path,\n",
    "    numberOfObjects=1,\n",
    "    size=456.7,\n",
    "    fileFormats=[\"json\"],\n",
    ")\n",
    "\n",
    "# Register with OpenMetadata\n",
    "file2_entity = om_con.create_or_update(data=file2_request)\n",
    "print(f\"✅ Container created: {file2_entity.fullyQualifiedName}\")"
   ],
   "id": "e7c9b24a6e34e9e7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Container created: root='Constances-Datalake.geospatial.vector.\"file2.geojson\"'\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T09:39:05.141533Z",
     "start_time": "2025-09-09T09:39:05.051384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# config file3\n",
    "file3_name = \"file3.tif\"\n",
    "file3_desc = \"AVG Temperature of Frace city by month in geotiff format\"\n",
    "file3_path = f\"{RASTER_PATH}/{file3_name}\"\n",
    "\n",
    "# define columns of file1\n",
    "file3_columns = [\n",
    "    Column(\n",
    "        name=\"avg_temperature\",\n",
    "        displayName=\"Average temperature\",\n",
    "        dataType=DataType.INT,\n",
    "        description=\"Average temperature of a pixel in geotiff\"\n",
    "    ),\n",
    "    Column(\n",
    "        name=\"pixel\",\n",
    "        displayName=\"pixel\",\n",
    "        dataType=DataType.STRING,\n",
    "        description=\"Pixel of french city in geotiff\"\n",
    "    ),\n",
    "\n",
    "]\n",
    "\n",
    "# Build the data model for the container\n",
    "file3_data_model = ContainerDataModel(\n",
    "    isPartitioned=False,\n",
    "    columns=file3_columns\n",
    ")\n",
    "\n",
    "# Create the container request\n",
    "file3_request = CreateContainerRequest(\n",
    "    name=file3_name,\n",
    "    displayName=file3_name,\n",
    "    description=file3_desc,\n",
    "    service=storage_service_entity.fullyQualifiedName,   # must be the FQN of your StorageService\n",
    "    parent=EntityReference(id=raster_dir_entity.id, type=\"container\"), # must be the parent container FQN\n",
    "    dataModel=file3_data_model,\n",
    "    fullPath=file3_path,\n",
    "    numberOfObjects=1,\n",
    "    size=789.7,\n",
    "    fileFormats=[\"csv\"],\n",
    ")\n",
    "\n",
    "# Register with OpenMetadata\n",
    "file3_entity = om_con.create_or_update(data=file3_request)\n",
    "print(f\"✅ Container created: {file3_entity.fullyQualifiedName}\")"
   ],
   "id": "b0eca7b19eddbe44",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Container created: root='Constances-Datalake.geospatial.raster.\"file3.tif\"'\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T09:41:50.284955Z",
     "start_time": "2025-09-09T09:41:50.089651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# config file4\n",
    "file4_name = \"file4.nc\"\n",
    "file4_desc = \"AVG air pollution of Frace city by month in netcdf format\"\n",
    "file4_path = f\"{RASTER_PATH}/{file4_name}\"\n",
    "\n",
    "# define columns of file4\n",
    "file4_columns = [\n",
    "    Column(\n",
    "        name=\"avg_air_pollution\",\n",
    "        displayName=\"Average air pollution\",\n",
    "        dataType=DataType.INT,\n",
    "        description=\"Average air pollution of a pixel in netcdf\"\n",
    "    ),\n",
    "    Column(\n",
    "        name=\"pixel\",\n",
    "        displayName=\"pixel\",\n",
    "        dataType=DataType.STRING,\n",
    "        description=\"Pixel of french city in netcdf\"\n",
    "    ),\n",
    "\n",
    "]\n",
    "\n",
    "# Build the data model for the container\n",
    "file4_data_model = ContainerDataModel(\n",
    "    isPartitioned=False,\n",
    "    columns=file4_columns\n",
    ")\n",
    "\n",
    "# Create the container request\n",
    "file4_request = CreateContainerRequest(\n",
    "    name=file4_name,\n",
    "    displayName=file4_name,\n",
    "    description=file4_desc,\n",
    "    service=storage_service_entity.fullyQualifiedName,   # must be the FQN of your StorageService\n",
    "    parent=EntityReference(id=raster_dir_entity.id, type=\"container\"), # must be the parent container FQN\n",
    "    dataModel=file4_data_model,\n",
    "    fullPath=file4_path,\n",
    "    numberOfObjects=1,\n",
    "    size=666.7,\n",
    "    fileFormats=[\"csv\"],\n",
    ")\n",
    "\n",
    "# Register with OpenMetadata\n",
    "file4_entity = om_con.create_or_update(data=file4_request)\n",
    "print(f\"✅ Container created: {file4_entity.fullyQualifiedName}\")"
   ],
   "id": "c6d16621ccb93dd7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Container created: root='Constances-Datalake.geospatial.raster.\"file4.nc\"'\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T09:56:41.194179Z",
     "start_time": "2025-09-09T09:56:41.182200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_file_entity(om_server_con,file_name:str, file_desc:str, file_path:str, file_columns, storage_service, parent_dir, file_size:float):\n",
    "    \"\"\"\n",
    "    This function will create a file entity(container) under a directory entity(container) inside a storage service.\n",
    "    :param om_server_con: open metadata server connection\n",
    "    :param file_name:\n",
    "    :param file_desc:\n",
    "    :param file_path:\n",
    "    :param file_columns: A list of columns\n",
    "    :param storage_service:\n",
    "    :param parent_dir:\n",
    "    :param file_size:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Build the data model for the container\n",
    "    file_data_model = ContainerDataModel(\n",
    "        isPartitioned=False,\n",
    "        columns=file_columns\n",
    "    )\n",
    "\n",
    "    # Create the container request\n",
    "    file_request = CreateContainerRequest(\n",
    "        name=file_name,\n",
    "        displayName=file_name,\n",
    "        description=file_desc,\n",
    "        service=storage_service.fullyQualifiedName,   # must be the FQN of your StorageService\n",
    "        parent=EntityReference(id=parent_dir.id, type=\"container\"), # must be the parent container FQN\n",
    "        dataModel=file_data_model,\n",
    "        fullPath=file_path,\n",
    "        numberOfObjects=1,\n",
    "        size=file_size,\n",
    "        fileFormats=[\"csv\"],\n",
    "    )\n",
    "\n",
    "    # Register with OpenMetadata\n",
    "    file_entity = om_server_con.create_or_update(data=file_request)\n",
    "    print(f\"✅ Container created: {file_entity.fullyQualifiedName}\")"
   ],
   "id": "255db05c61cb974",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T09:56:42.298643Z",
     "start_time": "2025-09-09T09:56:42.180938Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# config file5\n",
    "file5_name = \"file5.parquet\"\n",
    "file5_desc = \"blood test of a patient\"\n",
    "file5_path = f\"{RASTER_PATH}/{file5_name}\"\n",
    "file5_size = 345.6\n",
    "# define columns of file1\n",
    "file5_columns = [\n",
    "    Column(\n",
    "        name=\"blood_test_id\",\n",
    "        displayName=\"blood test id\",\n",
    "        dataType=DataType.INT,\n",
    "        description=\"Unique identifier of the blood test\"\n",
    "    ),\n",
    "    Column(\n",
    "        name=\"patient_name\",\n",
    "        displayName=\"patient Name\",\n",
    "        dataType=DataType.STRING,\n",
    "        description=\"Name of the patient\"\n",
    "    ),\n",
    "    Column(\n",
    "        name=\"red_cell_count\",\n",
    "        displayName=\"red cell number count\",\n",
    "        dataType=DataType.INT,\n",
    "        description=\"Number of red cells\"\n",
    "    ),\n",
    "\n",
    "]\n",
    "\n",
    "create_file_entity(om_con,file5_name,file5_desc,file5_path,file5_columns,storage_service_entity,clinical_dir_entity,file5_size)"
   ],
   "id": "1f1497041f93221",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Container created: root='Constances-Datalake.clinical.\"file5.parquet\"'\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T10:00:18.387900Z",
     "start_time": "2025-09-09T10:00:18.284728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# config file6\n",
    "file6_name = \"file6.csv\"\n",
    "file6_desc = \"general test of a patient\"\n",
    "file6_path = f\"{RASTER_PATH}/{file6_name}\"\n",
    "file6_size = 8888.8\n",
    "# define columns of file6\n",
    "file6_columns = [\n",
    "    Column(\n",
    "        name=\"general_test_id\",\n",
    "        displayName=\"general test id\",\n",
    "        dataType=DataType.INT,\n",
    "        description=\"Unique identifier of the general test\"\n",
    "    ),\n",
    "    Column(\n",
    "        name=\"patient_name\",\n",
    "        displayName=\"patient Name\",\n",
    "        dataType=DataType.STRING,\n",
    "        description=\"Name of the patient\"\n",
    "    ),\n",
    "    Column(\n",
    "        name=\"patient_weight\",\n",
    "        displayName=\"patient weight\",\n",
    "        dataType=DataType.INT,\n",
    "        description=\"weight of a patient\"\n",
    "    ),\n",
    "\n",
    "    Column(\n",
    "        name=\"patient_height\",\n",
    "        displayName=\"patient height\",\n",
    "        dataType=DataType.INT,\n",
    "        description=\"height of a patient\"\n",
    "    ),\n",
    "\n",
    "]\n",
    "\n",
    "create_file_entity(om_con,file6_name,file6_desc,file6_path,file6_columns,storage_service_entity,clinical_dir_entity,file6_size)"
   ],
   "id": "e3ec7bd5fc8aa9aa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Container created: root='Constances-Datalake.clinical.\"file6.csv\"'\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "\n",
   "id": "7f7b049c640aba2d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T16:07:20.157813Z",
     "start_time": "2025-09-08T16:07:20.080425Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "c49d31983e3ab053",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StorageService created: id=Uuid(root=UUID('36592ca7-8dda-4fbe-ab45-d31c592976a3')) name=EntityName(root='Local-Filesystem') fullyQualifiedName=FullyQualifiedEntityName(root='Local-Filesystem') displayName='Local Filesystem' serviceType=<StorageServiceType.CustomStorage: 'CustomStorage'> description=Markdown(root='Custom storage service for local files') connection=StorageConnection(config=CustomStorageConnection(type=<CustomStorageType.CustomStorage: 'CustomStorage'>, sourcePythonClass='metadata.ingestion.source.storage.local.LocalStorageSource', connectionOptions=ConnectionOptions(root={}), containerFilterPattern=None, supportsMetadataExtraction=SupportsMetadataExtraction(root=True))) pipelines=None testConnectionResult=None tags=[] version=EntityVersion(root=0.1) updatedAt=Timestamp(root=1757347640097) updatedBy='ingestion-bot' href=Href(root=AnyUrl('http://localhost:8585/v1/services/storageServices/36592ca7-8dda-4fbe-ab45-d31c592976a3')) owners=None changeDescription=None incrementalChangeDescription=None deleted=False dataProducts=None followers=None domains=None ingestionRunner=None\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T16:07:30.887582Z",
     "start_time": "2025-09-08T16:07:30.833642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from metadata.generated.schema.api.data.createContainer import CreateContainerRequest\n",
    "from metadata.generated.schema.entity.data.container import ContainerDataModel\n",
    "\n",
    "# 1. Create the root container (Drive)\n",
    "container_request = CreateContainerRequest(\n",
    "    name=\"local-bucket\",   # logical bucket name (must match your connectionOptions.bucketName if used)\n",
    "    displayName=\"Local Bucket\",\n",
    "    service=storage_service_entity.fullyQualifiedName,\n",
    "    dataModel=ContainerDataModel(\n",
    "        isPartitioned=False,\n",
    "        columns=[]\n",
    "    )\n",
    ")\n",
    "container_entity = om_con.create_or_update(data=container_request)\n",
    "print(f\"Container created: {container_entity.fullyQualifiedName}\")\n"
   ],
   "id": "87aa38221685eff6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Container created: root='Local-Filesystem.local-bucket'\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T08:04:41.923084Z",
     "start_time": "2025-09-09T08:04:41.726703Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from metadata.generated.schema.api.data.createContainer import CreateContainerRequest\n",
    "from metadata.generated.schema.entity.data.container import ContainerDataModel\n",
    "from metadata.generated.schema.entity.data.table import Column, DataType\n",
    "\n",
    "# Example columns for the container's data model\n",
    "columns = [\n",
    "    Column(\n",
    "        name=\"hospital_id\",\n",
    "        displayName=\"Hospital ID\",\n",
    "        dataType=DataType.INT,\n",
    "        description=\"Unique identifier for the hospital\"\n",
    "    ),\n",
    "    Column(\n",
    "        name=\"hospital_name\",\n",
    "        displayName=\"Hospital Name\",\n",
    "        dataType=DataType.STRING,\n",
    "        description=\"Name of the hospital\"\n",
    "    ),\n",
    "    Column(\n",
    "        name=\"city\",\n",
    "        displayName=\"City\",\n",
    "        dataType=DataType.STRING,\n",
    "        description=\"City where the hospital is located\"\n",
    "    ),\n",
    "    Column(\n",
    "        name=\"capacity\",\n",
    "        displayName=\"Capacity\",\n",
    "        dataType=DataType.INT,\n",
    "        description=\"Number of beds available\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Build the data model for the container\n",
    "data_model = ContainerDataModel(\n",
    "    isPartitioned=False,\n",
    "    columns=columns\n",
    ")\n",
    "\n",
    "# Create the container request\n",
    "container_request = CreateContainerRequest(\n",
    "    name=\"hospital_in_france\",\n",
    "    displayName=\"Hospital Data Container\",\n",
    "    description=\"All hospital data in France\",\n",
    "    service=\"Local-Filesystem\",   # must be the FQN of your StorageService\n",
    "    dataModel=data_model,\n",
    "    prefix=\"/INSERM/geo/datasets/raw/hospitals\",\n",
    "    numberOfObjects=1,\n",
    "    size=123.4,\n",
    "    fileFormats=[\"csv\"],\n",
    ")\n",
    "\n",
    "# Register with OpenMetadata\n",
    "container_entity = om_con.create_or_update(data=container_request)\n",
    "print(f\"✅ Container created: {container_entity.fullyQualifiedName}\")\n"
   ],
   "id": "f50d6574dce5c43d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Container created: root='Local-Filesystem.hospital_in_france'\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T08:13:19.452931Z",
     "start_time": "2025-09-09T08:13:19.180856Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from metadata.generated.schema.type.entityReference import EntityReference\n",
    "\n",
    "patients_columns = [\n",
    "    Column(name=\"patient_id\", dataType=DataType.INT, description=\"Unique patient ID\"),\n",
    "    Column(name=\"name\", dataType=DataType.STRING, description=\"Patient full name\"),\n",
    "    Column(name=\"dob\", dataType=DataType.DATE, description=\"Date of birth\"),\n",
    "]\n",
    "\n",
    "staff_columns = [\n",
    "    Column(name=\"staff_id\", dataType=DataType.INT, description=\"Unique staff ID\"),\n",
    "    Column(name=\"name\", dataType=DataType.STRING, description=\"Staff member full name\"),\n",
    "    Column(name=\"role\", dataType=DataType.STRING, description=\"Role in the hospital\"),\n",
    "]\n",
    "\n",
    "patients_model = ContainerDataModel(isPartitioned=False, columns=patients_columns)\n",
    "staff_model = ContainerDataModel(isPartitioned=False, columns=staff_columns)\n",
    "\n",
    "# -------------------------\n",
    "# 3. Create child containers with parent reference\n",
    "# -------------------------\n",
    "patients_request = CreateContainerRequest(\n",
    "    name=\"patients\",\n",
    "    displayName=\"Patients Data\",\n",
    "    description=\"Container for patient records\",\n",
    "    service=\"Local-Filesystem\",\n",
    "    parent=EntityReference(id=container_entity.id, type=\"container\"),\n",
    "    dataModel=patients_model,\n",
    "    prefix=\"/INSERM/geo/datasets/hospitals/patients\",\n",
    ")\n",
    "\n",
    "staff_request = CreateContainerRequest(\n",
    "    name=\"staff\",\n",
    "    displayName=\"Staff Data\",\n",
    "    description=\"Container for staff records\",\n",
    "    service=\"Local-Filesystem\",\n",
    "    parent=EntityReference(id=container_entity.id, type=\"container\"),\n",
    "    dataModel=staff_model,\n",
    "    prefix=\"/INSERM/geo/datasets/hospitals/staff\",\n",
    ")\n",
    "\n",
    "patients_container = om_con.create_or_update(data=patients_request)\n",
    "staff_container = om_con.create_or_update(data=staff_request)"
   ],
   "id": "8ba64f7ee98a75c3",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T16:11:46.741886Z",
     "start_time": "2025-09-08T16:11:46.627529Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from metadata.generated.schema.api.data.createFile import CreateFileRequest\n",
    "from metadata.generated.schema.entity.data.file import FileType\n",
    "\n",
    "\n",
    "file_request = CreateFileRequest(\n",
    "    name=\"hospitals.csv\",\n",
    "    displayName=\"Hospitals in France\",\n",
    "    description=\"Dataset of French hospitals with location information\",\n",
    "    service=container_entity.fullyQualifiedName,      # points to the Container\n",
    "    fileType=FileType.CSV,\n",
    "    mimeType=\"text/csv\",\n",
    "    fileExtension=\".csv\",\n",
    "    path=\"/INSERM/geo/datasets/raw/hospitals.csv\",\n",
    "    size=123456,  # in bytes\n",
    "    checksum=\"d41d8cd98f00b204e9800998ecf8427e\",  # optional md5/sha1/sha256 hash\n",
    "    isShared=False,\n",
    "    fileVersion=\"v1.0\",\n",
    ")\n",
    "\n",
    "file_entity = om_con.create_or_update(data=file_request)\n",
    "print(f\"File created: {file_entity.fullyQualifiedName}\")\n"
   ],
   "id": "50233f670b6a0d8",
   "outputs": [
    {
     "ename": "APIError",
     "evalue": "driveService instance for \"Local-Filesystem.local-bucket\" not found",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mHTTPError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\git\\Seminare_data_catalog\\dc_venv\\Lib\\site-packages\\metadata\\ingestion\\ometa\\client.py:261\u001B[39m, in \u001B[36mREST._one_request\u001B[39m\u001B[34m(self, method, url, opts, retry)\u001B[39m\n\u001B[32m    260\u001B[39m resp = \u001B[38;5;28mself\u001B[39m._session.request(method, url, **opts)\n\u001B[32m--> \u001B[39m\u001B[32m261\u001B[39m \u001B[43mresp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mraise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    263\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m resp.text != \u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\git\\Seminare_data_catalog\\dc_venv\\Lib\\site-packages\\requests\\models.py:1026\u001B[39m, in \u001B[36mResponse.raise_for_status\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1025\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m http_error_msg:\n\u001B[32m-> \u001B[39m\u001B[32m1026\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(http_error_msg, response=\u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[31mHTTPError\u001B[39m: 404 Client Error: Not Found for url: http://om-dev.casd.local/api/v1/drives/files",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mAPIError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[12]\u001B[39m\u001B[32m, line 20\u001B[39m\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mmetadata\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mgenerated\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mschema\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mentity\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mdata\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mfile\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m FileType\n\u001B[32m      5\u001B[39m file_request = CreateFileRequest(\n\u001B[32m      6\u001B[39m     name=\u001B[33m\"\u001B[39m\u001B[33mhospitals.csv\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m      7\u001B[39m     displayName=\u001B[33m\"\u001B[39m\u001B[33mHospitals in France\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m     17\u001B[39m     fileVersion=\u001B[33m\"\u001B[39m\u001B[33mv1.0\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     18\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m20\u001B[39m file_entity = \u001B[43mom_con\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcreate_or_update\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfile_request\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     21\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mFile created: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile_entity.fullyQualifiedName\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\git\\Seminare_data_catalog\\dc_venv\\Lib\\site-packages\\metadata\\ingestion\\ometa\\ometa_api.py:341\u001B[39m, in \u001B[36mOpenMetadata.create_or_update\u001B[39m\u001B[34m(self, data)\u001B[39m\n\u001B[32m    339\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcreate_or_update\u001B[39m(\u001B[38;5;28mself\u001B[39m, data: C) -> T:\n\u001B[32m    340\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Run a PUT requesting via create request C\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m341\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_create\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mput\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\git\\Seminare_data_catalog\\dc_venv\\Lib\\site-packages\\metadata\\ingestion\\ometa\\ometa_api.py:328\u001B[39m, in \u001B[36mOpenMetadata._create\u001B[39m\u001B[34m(self, data, method)\u001B[39m\n\u001B[32m    323\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m InvalidEntityException(\n\u001B[32m    324\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mPUT operations need a CreateEntity, not \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mentity\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    325\u001B[39m     )\n\u001B[32m    327\u001B[39m fn = \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m.client, method)\n\u001B[32m--> \u001B[39m\u001B[32m328\u001B[39m resp = \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    329\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# this might be a regular pydantic model so we build the context manually\u001B[39;49;00m\n\u001B[32m    330\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mget_suffix\u001B[49m\u001B[43m(\u001B[49m\u001B[43mentity\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    331\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmodel_dump_json\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m=\u001B[49m\u001B[43m{\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmask_secrets\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mby_alias\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    332\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    333\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m resp:\n\u001B[32m    334\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m EmptyPayloadException(\n\u001B[32m    335\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mGot an empty response when trying to PUT to \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.get_suffix(entity)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdata.model_dump_json()\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    336\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\git\\Seminare_data_catalog\\dc_venv\\Lib\\site-packages\\metadata\\utils\\execution_time_tracker.py:195\u001B[39m, in \u001B[36mcalculate_execution_time.<locals>.decorator.<locals>.inner\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    192\u001B[39m execution_time = ExecutionTimeTracker()\n\u001B[32m    194\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m execution_time(context \u001B[38;5;129;01mor\u001B[39;00m func.\u001B[34m__name__\u001B[39m, store):\n\u001B[32m--> \u001B[39m\u001B[32m195\u001B[39m     result = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    197\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\git\\Seminare_data_catalog\\dc_venv\\Lib\\site-packages\\metadata\\ingestion\\ometa\\client.py:349\u001B[39m, in \u001B[36mREST.put\u001B[39m\u001B[34m(self, path, data)\u001B[39m\n\u001B[32m    337\u001B[39m \u001B[38;5;129m@calculate_execution_time\u001B[39m(context=\u001B[33m\"\u001B[39m\u001B[33mPUT\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    338\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mput\u001B[39m(\u001B[38;5;28mself\u001B[39m, path, data=\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[32m    339\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    340\u001B[39m \u001B[33;03m    PUT method\u001B[39;00m\n\u001B[32m    341\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    347\u001B[39m \u001B[33;03m        Response\u001B[39;00m\n\u001B[32m    348\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m349\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_request\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mPUT\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\git\\Seminare_data_catalog\\dc_venv\\Lib\\site-packages\\metadata\\ingestion\\ometa\\client.py:230\u001B[39m, in \u001B[36mREST._request\u001B[39m\u001B[34m(self, method, path, data, json, base_url, api_version, headers)\u001B[39m\n\u001B[32m    228\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m retry >= \u001B[32m0\u001B[39m:\n\u001B[32m    229\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m230\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_one_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretry\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    231\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m LimitsException \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[32m    232\u001B[39m         logger.error(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mFeature limit exceeded for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00murl\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\git\\Seminare_data_catalog\\dc_venv\\Lib\\site-packages\\metadata\\ingestion\\ometa\\client.py:287\u001B[39m, in \u001B[36mREST._one_request\u001B[39m\u001B[34m(self, method, url, opts, retry)\u001B[39m\n\u001B[32m    285\u001B[39m     error = resp.json()\n\u001B[32m    286\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mcode\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m error:\n\u001B[32m--> \u001B[39m\u001B[32m287\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m APIError(error, http_error) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mhttp_error\u001B[39;00m\n\u001B[32m    288\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    289\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[31mAPIError\u001B[39m: driveService instance for \"Local-Filesystem.local-bucket\" not found"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d941f1a10beaae84"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T08:24:49.732805Z",
     "start_time": "2025-09-09T08:24:49.489821Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -------------------------\n",
    "# 1. Create root container\n",
    "# -------------------------\n",
    "root_request = CreateContainerRequest(\n",
    "    name=\"root_container\",\n",
    "    displayName=\"Root Directory\",\n",
    "    description=\"Top-level root container\",\n",
    "    service=\"Local-Filesystem\",  # StorageService FQN\n",
    "    prefix=\"/\",\n",
    ")\n",
    "root_container = om_con.create_or_update(data=root_request)\n",
    "\n",
    "# -------------------------\n",
    "# 2. Create sub-container dir1\n",
    "# -------------------------\n",
    "dir1_request = CreateContainerRequest(\n",
    "    name=\"dir1\",\n",
    "    displayName=\"Directory 1\",\n",
    "    description=\"Sub-directory under root\",\n",
    "    service=\"Local-Filesystem\",\n",
    "    parent=EntityReference(id=root_container.id, type=\"container\"),\n",
    "    prefix=\"/dir1\",\n",
    ")\n",
    "dir1_container = om_con.create_or_update(data=dir1_request)\n",
    "\n",
    "# -------------------------\n",
    "# 3. Add files under dir1\n",
    "# -------------------------\n",
    "file1_request = CreateContainerRequest(\n",
    "    name=\"file1.csv\",\n",
    "    displayName=\"file1.csv\",\n",
    "    description=\"Sample CSV file1\",\n",
    "    service=\"Local-Filesystem\",\n",
    "    parent=EntityReference(id=dir1_container.id, type=\"container\"),\n",
    "    prefix=\"/dir1/file1.csv\",\n",
    "    size=1024,\n",
    ")\n",
    "\n",
    "file2_request = CreateContainerRequest(\n",
    "    name=\"file2.json\",\n",
    "    displayName=\"file2.json\",\n",
    "    description=\"Sample JSON file2\",\n",
    "    service=\"Local-Filesystem\",\n",
    "    parent=EntityReference(id=dir1_container.id, type=\"container\"),\n",
    "    prefix=\"/dir1/file1.csv\",\n",
    "    size=2048,\n",
    ")\n",
    "\n",
    "file1_entity = om_con.create_or_update(data=file1_request)\n",
    "file2_entity = om_con.create_or_update(data=file2_request)"
   ],
   "id": "f82c217f9da9ed69",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T07:34:53.026360Z",
     "start_time": "2025-09-09T07:34:52.904104Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from metadata.generated.schema.security.credentials.awsCredentials import AWSCredentials\n",
    "from metadata.generated.schema.api.services.createStorageService import CreateStorageServiceRequest\n",
    "from metadata.generated.schema.entity.services.storageService import StorageServiceType, StorageConnection\n",
    "from metadata.generated.schema.entity.services.connections.storage.s3Connection import S3Connection\n",
    "from metadata.generated.schema.api.data.createContainer import CreateContainerRequest\n",
    "from metadata.generated.schema.entity.data.container import ContainerDataModel\n",
    "from metadata.generated.schema.api.data.createFile import CreateFileRequest\n",
    "\n",
    "# 1. Create S3 Storage Service\n",
    "s3_conn = S3Connection(\n",
    "    awsConfig=AWSCredentials(\n",
    "        awsAccessKeyId=\"YOUR_ACCESS_KEY\",\n",
    "        awsSecretAccessKey=\"YOUR_SECRET_KEY\",\n",
    "        awsRegion=\"us-east-1\",\n",
    "        assumeRoleArn=None\n",
    "    ),\n",
    "    bucketNames=[\"my-bucket\"]  # must be a list\n",
    ")\n",
    "\n",
    "storage_service_req = CreateStorageServiceRequest(\n",
    "    name=\"MyS3Service\",\n",
    "    serviceType=StorageServiceType.S3,\n",
    "    connection=StorageConnection(config=s3_conn),\n",
    "    description=\"Metadata for AWS S3 data lake\"\n",
    ")\n",
    "storage_service_entity = om_con.create_or_update(data=storage_service_req)\n",
    "\n",
    "# 2. (Optional) Create a container for a prefix like 'raw/'\n",
    "container_req = CreateContainerRequest(\n",
    "    name=\"raw\",\n",
    "    displayName=\"Raw files\",\n",
    "    service=storage_service_entity.fullyQualifiedName,\n",
    "    dataModel=ContainerDataModel(isPartitioned=False,columns=[])\n",
    ")\n",
    "\n",
    "container_entity = om_con.create_or_update(data=container_req)\n",
    "\n"
   ],
   "id": "21f828af3762c2aa",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T07:45:33.248321Z",
     "start_time": "2025-09-09T07:45:33.016986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from metadata.generated.schema.entity.services.connections.drive.customDriveConnection import CustomDriveConnection\n",
    "from metadata.generated.schema.api.services.createDriveService import CreateDriveServiceRequest\n",
    "from metadata.generated.schema.entity.services.driveService import DriveServiceType, DriveConnection\n",
    "from metadata.generated.schema.security.credentials.awsCredentials import AWSCredentials\n",
    "\n",
    "# S3 connection config\n",
    "custom_conn = CustomDriveConnection(\n",
    ")\n",
    "\n",
    "drive_service_request = CreateDriveServiceRequest(\n",
    "    name=\"MyS3Service\",\n",
    "    serviceType=DriveServiceType.CustomDrive,\n",
    "    connection=DriveConnection(config=custom_conn),\n",
    "    description=\"S3 storage service (legacy Drive API)\"\n",
    ")\n",
    "\n",
    "drive_service_entity = om_con.create_or_update(data=drive_service_request)\n",
    "print(f\"✅ DriveService created: {drive_service_entity.fullyQualifiedName}\")"
   ],
   "id": "e64ac7c3c57373ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DriveService created: root='MyS3Service'\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "\n",
   "id": "eaffe7596b45f2c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T07:49:32.573678Z",
     "start_time": "2025-09-09T07:49:32.326740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. (Optional) Add a single file manually\n",
    "file_req = CreateFileRequest(\n",
    "    name=\"example.csv\",\n",
    "    service=container_entity.fullyQualifiedName,\n",
    "    fileType=\"CSV\",\n",
    "    path=\"raw/example.csv\",\n",
    "    size=1024\n",
    ")\n",
    "om_con.create_or_update(data=file_req)"
   ],
   "id": "c2b05c1cd0ed0411",
   "outputs": [
    {
     "ename": "APIError",
     "evalue": "driveService instance for \"MyS3Service.raw\" not found",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mHTTPError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\git\\Seminare_data_catalog\\dc_venv\\Lib\\site-packages\\metadata\\ingestion\\ometa\\client.py:261\u001B[39m, in \u001B[36mREST._one_request\u001B[39m\u001B[34m(self, method, url, opts, retry)\u001B[39m\n\u001B[32m    260\u001B[39m resp = \u001B[38;5;28mself\u001B[39m._session.request(method, url, **opts)\n\u001B[32m--> \u001B[39m\u001B[32m261\u001B[39m \u001B[43mresp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mraise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    263\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m resp.text != \u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\git\\Seminare_data_catalog\\dc_venv\\Lib\\site-packages\\requests\\models.py:1026\u001B[39m, in \u001B[36mResponse.raise_for_status\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1025\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m http_error_msg:\n\u001B[32m-> \u001B[39m\u001B[32m1026\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(http_error_msg, response=\u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[31mHTTPError\u001B[39m: 404 Client Error: Not Found for url: http://om-dev.casd.local/api/v1/drives/files",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mAPIError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[12]\u001B[39m\u001B[32m, line 9\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# 3. (Optional) Add a single file manually\u001B[39;00m\n\u001B[32m      2\u001B[39m file_req = CreateFileRequest(\n\u001B[32m      3\u001B[39m     name=\u001B[33m\"\u001B[39m\u001B[33mexample.csv\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m      4\u001B[39m     service=container_entity.fullyQualifiedName,\n\u001B[32m   (...)\u001B[39m\u001B[32m      7\u001B[39m     size=\u001B[32m1024\u001B[39m\n\u001B[32m      8\u001B[39m )\n\u001B[32m----> \u001B[39m\u001B[32m9\u001B[39m \u001B[43mom_con\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcreate_or_update\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfile_req\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\git\\Seminare_data_catalog\\dc_venv\\Lib\\site-packages\\metadata\\ingestion\\ometa\\ometa_api.py:341\u001B[39m, in \u001B[36mOpenMetadata.create_or_update\u001B[39m\u001B[34m(self, data)\u001B[39m\n\u001B[32m    339\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcreate_or_update\u001B[39m(\u001B[38;5;28mself\u001B[39m, data: C) -> T:\n\u001B[32m    340\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Run a PUT requesting via create request C\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m341\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_create\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mput\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\git\\Seminare_data_catalog\\dc_venv\\Lib\\site-packages\\metadata\\ingestion\\ometa\\ometa_api.py:328\u001B[39m, in \u001B[36mOpenMetadata._create\u001B[39m\u001B[34m(self, data, method)\u001B[39m\n\u001B[32m    323\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m InvalidEntityException(\n\u001B[32m    324\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mPUT operations need a CreateEntity, not \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mentity\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    325\u001B[39m     )\n\u001B[32m    327\u001B[39m fn = \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m.client, method)\n\u001B[32m--> \u001B[39m\u001B[32m328\u001B[39m resp = \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    329\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# this might be a regular pydantic model so we build the context manually\u001B[39;49;00m\n\u001B[32m    330\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mget_suffix\u001B[49m\u001B[43m(\u001B[49m\u001B[43mentity\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    331\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmodel_dump_json\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m=\u001B[49m\u001B[43m{\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmask_secrets\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mby_alias\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    332\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    333\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m resp:\n\u001B[32m    334\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m EmptyPayloadException(\n\u001B[32m    335\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mGot an empty response when trying to PUT to \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.get_suffix(entity)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdata.model_dump_json()\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    336\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\git\\Seminare_data_catalog\\dc_venv\\Lib\\site-packages\\metadata\\utils\\execution_time_tracker.py:195\u001B[39m, in \u001B[36mcalculate_execution_time.<locals>.decorator.<locals>.inner\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    192\u001B[39m execution_time = ExecutionTimeTracker()\n\u001B[32m    194\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m execution_time(context \u001B[38;5;129;01mor\u001B[39;00m func.\u001B[34m__name__\u001B[39m, store):\n\u001B[32m--> \u001B[39m\u001B[32m195\u001B[39m     result = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    197\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\git\\Seminare_data_catalog\\dc_venv\\Lib\\site-packages\\metadata\\ingestion\\ometa\\client.py:349\u001B[39m, in \u001B[36mREST.put\u001B[39m\u001B[34m(self, path, data)\u001B[39m\n\u001B[32m    337\u001B[39m \u001B[38;5;129m@calculate_execution_time\u001B[39m(context=\u001B[33m\"\u001B[39m\u001B[33mPUT\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    338\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mput\u001B[39m(\u001B[38;5;28mself\u001B[39m, path, data=\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[32m    339\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    340\u001B[39m \u001B[33;03m    PUT method\u001B[39;00m\n\u001B[32m    341\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    347\u001B[39m \u001B[33;03m        Response\u001B[39;00m\n\u001B[32m    348\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m349\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_request\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mPUT\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\git\\Seminare_data_catalog\\dc_venv\\Lib\\site-packages\\metadata\\ingestion\\ometa\\client.py:230\u001B[39m, in \u001B[36mREST._request\u001B[39m\u001B[34m(self, method, path, data, json, base_url, api_version, headers)\u001B[39m\n\u001B[32m    228\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m retry >= \u001B[32m0\u001B[39m:\n\u001B[32m    229\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m230\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_one_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretry\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    231\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m LimitsException \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[32m    232\u001B[39m         logger.error(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mFeature limit exceeded for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00murl\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\git\\Seminare_data_catalog\\dc_venv\\Lib\\site-packages\\metadata\\ingestion\\ometa\\client.py:287\u001B[39m, in \u001B[36mREST._one_request\u001B[39m\u001B[34m(self, method, url, opts, retry)\u001B[39m\n\u001B[32m    285\u001B[39m     error = resp.json()\n\u001B[32m    286\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mcode\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m error:\n\u001B[32m--> \u001B[39m\u001B[32m287\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m APIError(error, http_error) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mhttp_error\u001B[39;00m\n\u001B[32m    288\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    289\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[31mAPIError\u001B[39m: driveService instance for \"MyS3Service.raw\" not found"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Clean up",
   "id": "ee2443979be2930f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T12:18:09.084555Z",
     "start_time": "2025-09-08T12:18:01.185744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# get database service id\n",
    "service_id = om_con.get_by_name(\n",
    "    entity=DatabaseService, fqn=DB_SERVICE_NAME\n",
    ").id\n",
    "\n",
    "# delete service by using id\n",
    "om_con.delete(\n",
    "    entity=DatabaseService,\n",
    "    entity_id=service_id,\n",
    "    recursive=True,\n",
    "    hard_delete=True,\n",
    ")"
   ],
   "id": "f58adb42567775ef",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T15:21:41.409870Z",
     "start_time": "2025-09-08T15:21:41.350300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# get file system service id\n",
    "\n",
    "\n",
    "STORE_SER_NAME = \"Local-Filesystem\"\n",
    "storage_service_entity = om_con.get_by_name(\n",
    "    entity=StorageService, fqn=STORE_SER_NAME\n",
    ")\n",
    "print(storage_service_entity)"
   ],
   "id": "20549119fa56f219",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id=Uuid(root=UUID('b9e113ba-2ea4-4d75-9713-38d90ce7bf98')) name=EntityName(root='Local-Filesystem') fullyQualifiedName=FullyQualifiedEntityName(root='Local-Filesystem') displayName='Local Filesystem' serviceType=<StorageServiceType.CustomStorage: 'CustomStorage'> description=Markdown(root='Local filesystem containing raw data files.') connection=StorageConnection(config=CustomStorageConnection(type=<CustomStorageType.CustomStorage: 'CustomStorage'>, sourcePythonClass='metadata.ingestion.source.storage.local.LocalStorageSource', connectionOptions=ConnectionOptions(root={'bucketName': 'local-bucket', 'configSource': '/data'}), containerFilterPattern=None, supportsMetadataExtraction=SupportsMetadataExtraction(root=True))) pipelines=None testConnectionResult=None tags=[] version=EntityVersion(root=0.2) updatedAt=Timestamp(root=1757340315204) updatedBy='ingestion-bot' href=Href(root=AnyUrl('http://localhost:8585/v1/services/storageServices/b9e113ba-2ea4-4d75-9713-38d90ce7bf98')) owners=None changeDescription=ChangeDescription(fieldsAdded=[FieldChange(name=FieldName(root='displayName'), oldValue=None, newValue='Local Filesystem')], fieldsUpdated=[FieldChange(name=FieldName(root='connection'), oldValue='\"old-encrypted-value\"', newValue='\"new-encrypted-value\"')], fieldsDeleted=[], previousVersion=EntityVersion(root=0.1), changeSummary=ChangeSummaryModel(root={})) incrementalChangeDescription=ChangeDescription(fieldsAdded=[FieldChange(name=FieldName(root='displayName'), oldValue=None, newValue='Local Filesystem')], fieldsUpdated=[FieldChange(name=FieldName(root='connection'), oldValue='\"old-encrypted-value\"', newValue='\"new-encrypted-value\"')], fieldsDeleted=[], previousVersion=EntityVersion(root=0.1), changeSummary=ChangeSummaryModel(root={})) deleted=False dataProducts=None followers=None domains=None ingestionRunner=None\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T15:22:13.471686Z",
     "start_time": "2025-09-08T15:22:10.718824Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# delete service by using id\n",
    "om_con.delete(\n",
    "    entity=StorageService,\n",
    "    entity_id=storage_service_entity.id,\n",
    "    recursive=True,\n",
    "    hard_delete=True,\n",
    ")"
   ],
   "id": "1ffe8c25038a15ab",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8ab394b7a8dd2a96"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
