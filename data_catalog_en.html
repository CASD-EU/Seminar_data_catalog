<!DOCTYPE html>
<html lang="en" xmlns="http://www.w3.org/1999/html">

<head>
    <meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"/>

    <title>Spark and Sedona at CASD</title>

    <link rel="stylesheet" href="dist/reset.css"/>
    <link rel="stylesheet" href="dist/reveal.css"/>
    <link rel="stylesheet" href="dist/theme/dracula.css"/>

    <!-- Theme used for syntax highlighted code -->
    <link rel="stylesheet" href="plugin/highlight/monokai.css"/>
</head>

<body>
<div class="reveal">
    <div class="slides">
        <!--Slide 1: Overview-->
        <section>
            <section>

                <img src="assets/CASD.png" alt="casd logo" style="
                  height: 250px;
                  margin: 0 auto 4rem auto;
                  background: transparent;
                  -webkit-filter: invert(1);
                  filter: invert(1);
                " class="demo-logo"/>

                <h3>Spark and Sedona at CASD</h3>
                <p>
                    <small>Datascience team</small>
                </p>
            </section>

            <section data-transition="fade-in slide-out">
                <h3>Goals</h3>
                <ul>
                    <li>What is apache spark?</li>
                    <li>Data processing with spark.</li>
                    <li>What is apache sedona?</li>
                    <li>Spatial data processing with sedona.</li>
                </ul>
            </section>
        </section>

        <!--Slide 2: Why use python?-->
        <section data-transition="fade-in slide-out" class="left-align">
            <section data-align="left">
                <h3>Why use Apache Spark?</h3>

                <p>
                    <em>Apache Spark</em> is a <span style="font-weight: bold;">distributed computing framework</span>
                    designed for processing and
                    analyzing large-scale data efficiently. Spark is an <span style="font-weight: bold;">open-source project under the Apache Software
                    Foundation</span>.
                </p>

                <p>
                    <em>Apache Spark</em> is designed to replace the older distributed computing framework <span
                        style="font-weight: bold;">Hadoop</span>.
                    By default, spark performs data computation in memory, not on disk.
                </p>
                <small>Spark is now one of the most widely used big data processing engines.</small>


            </section>
            <section>
                <h3>Spark Key Advantages:</h3>

                <ul>
                    <li class="fragment"><em>Speed</em>: 100 times faster than Hadoop MapReduce</li>
                    <li class="fragment"><em>Versatility</em>: Batch data processing, Machine learning, Graph analytics
                    </li>
                    <li class="fragment"><em>Scalability</em>: Single machine, on-premise clusters, public cloud</li>
                    <li class="fragment"><em>Interoperability</em>: Python, Java, Scala, R and SQL</li>
                    <li class="fragment"><em>Vast and mature ecosystem</em></li>
                </ul>

                <aside class="notes">

                </aside>
            </section>
            <section id="disadvantages">
                <h3>Some disadvantages</h3>
                <ul>
                    <li class="fragment"><em>Requires significant hardware resources</em>: CPU, RAM and network(for
                        cluster mode).
                    </li>
                    <li class="fragment"><em>Requires complex configuration</em>: vast expertise to set up spark cluster
                        correctly.
                    </li>
                    <li class="fragment"><em>Deep learning curve</em>: RDDs, DataFrames, DAG scheduling, partitioning.
                    </li>
                    <li class="fragment"><em>Overhead for Small Jobs</em>: Spark session setup and task scheduling
                        introduce overhead.
                    </li>

                </ul>
            </section>

        </section>

        <!--Slide 3: Key concepts in spark-->
        <section data-transition="fade-in slide-out">
            <!--Slide 3-1: spark general architecture-->
            <section>
                <h3>Key Concepts in spark Application</h3>
                <p>Spark Application contains a <span style="font-weight: bold;">driver program</span> and <span
                        style="font-weight: bold;">data processing tasks</span> on a cluster.
                </p>
                <img src="assets/apache-spark-architecture.png" alt="apache-spark-architecture.png"/>
            </section>
            <!--Slide 3-2: key concept in spark 1-->
            <section>
                <h3>Kep concepts in spark(1):</h3>

                <ul>

                    <li class="fragment"><em>Driver</em>: Coordinates with the cluster manager and worker, translate
                        tasks into the execution plan, sends tasks to executors.
                    </li>

                    <li class="fragment"><em>Spark Session & Contexts</em>: Entry point of spark application, provides
                        access
                        to DataFrames, SQL queries, and configurations.
                    </li>

                    <li class="fragment"><em>Cluster manager</em>: Allocates CPU and RAM, determines which task runs
                        where and when, monitors tasks and replaces failed ones.
                    </li>

                    <li class="fragment"><em>Executor</em>: Run tasks and store data partitions in memory/disk.
                    </li>

                </ul>
            </section>
            <!--Slide 3-3: key concept in spark 2-->
            <section>
                <h3>Kep concepts in spark(2):</h3>

                <ul>

                    <li class="fragment"><em>RDD (Resilient Distributed Dataset)</em>: Core data model in spark.
                        A <span style="font-weight: bold;">distributed fault tolerance</span> collection of immutable
                        objects.
                    </li>
                    <li class="fragment"><em>DataFrame</em>: An overlay of RDD, provides sql like operations and
                        optimization(Catalyst, Tungsten)
                    </li>
                    <li class="fragment"><em>Dataset</em>: Type-safe version of DataFrame, only available for Scala/Java
                        API.
                    </li>
                    <li class="fragment"><em>Partitioning</em>: Data(RDD, DataFrame, Dataset) is split into partitions
                        distributed across executors.
                    </li>

                </ul>
            </section>

            <!--Slide 3-4: key concept in spark 3-->
            <section>
                <h3>Kep concepts in spark(3):</h3>

                <ul>
                    <li class="fragment"><em>Driver program</em>: Directed acyclic graph (DAG) of transformations and
                        actions.
                    </li>
                    <li class="fragment"><em>Transformations</em>: map, filter, groupBy, etc.
                    </li>
                    <li class="fragment"><em>Actions</em>: trigger the execution of the transformations before.
                    </li>
                    <li class="fragment"><em>Lazy Evaluation</em>: Transformations are not executed immediately.
                        Execution happens only when an action is called.
                        Allows Spark to optimize the DAG as much as possible.
                    </li>
                </ul>
            </section>

            <!--Slide 3-5: key concept in spark(advance)-->
            <section>
                <h3>Kep concepts in spark(advance):</h3>

                <small><p><em>Data shuffling</em> is the redistribution of data across executors. It's one of the most
                    expensive operations, <em>minimizing shuffle is the key for performance.</em>
                </p></small>
                <img src="assets/spark_shuffle.png" alt="spark_shuffle.png"/>

            </section>
        </section>


        <!--Slide 4: spark in CASD-->
        <section data-transition="fade-in slide-out">
            <!-- Slide 4-1: overview      -->
            <section>
                <h2>Spark in CASD</h2>

                <ul>
                    <li class="fragment"><em>Spark cluster mode</em>: local, and yarn with hdfs</li>
                    <li class="fragment"><em>Spark client API</em>: Python, Java, Scala, R, SQL</li>
                    <li class="fragment"><em>Integrated Development Environment (IDE)</em>: vs-code, r-studio, jupyter
                        notebook
                    </li>
                    <li class="fragment"><em>Project Structure & Configuration Files</em>: CASD best practices</li>
                </ul>
            </section>
            <!-- Slide 4-2: Install spark in casd      -->
            <section>
                <h3>Install spark in CASD</h3>
                <pre data-id="code-animation"><code class="language-Bash" data-trim
                                                    data-line-numbers="|3-9|27-32|38-41|"><script
                        type="text/template">
# step1: Install the latest spark framework
# goto the target folder
cd C:\Users\Public\Desktop\Raccourcis\Spark

# run the installation script
.\InstallSpark.ps1

# check the installed spark version
spark-shell --version

## it may take a few seconds to show the output, be patient
# expected output
Welcome to
____              __
/ __/__  ___ _____/ /__
_\ \/ _ \/ _ `/ __/  '_/
/___/ .__/\_,_/_/ /_/\_\   version 3.5.5
/_/

Using Scala version 2.12.18, OpenJDK 64-Bit Server VM, 11.0.2
Branch HEAD
Compiled by user ubuntu on 2024-08-06T11:36:15Z
Revision bb7846dd487f259994fdc69e18e03382e3f64f42
Url https://github.com/apache/spark
Type --help for more information.

# step2: Install the pyspark support
# create a python virtual environment
conda create --name spark-sedona python --offline

# activate the python virtual environment
conda activate spark-sedona

# check python version
python -V
# check installed packages
pip list
# install pyspark
pip install pyspark==3.5.5
# show the installed package version
pip show pyspark



					</script></code></pre>

            </section>
        </section>


        <!--Slide 5: Sedona introduction-->
        <section>
            <section>
                <h2>Why use Apache Sedona?</h2>
                <p><em>Apache Sedona</em> extends existing cluster computing systems, such as <span
                        style="font-weight: bold;">Apache Spark, Apache Flink, and Snowflake</span> for processing
                    large-scale spatial data.

                    It provides <span style="font-weight: bold;">distributed Spatial Datasets and Spatial SQL query engine</span>
                    that efficiently load, process, and analyze large-scale spatial data across machines.</p>

                <p>CASD provides <em>Apache Spark cluster for Sedona</em>.</p>
            </section>
           <!--Slide 5-2: Sedona advantages-->
            <section data-align="left">
                <h3>Sedona Key Advantages:</h3>

                <ul>
                    <li class="fragment"><em>Speed</em>: 10-100 times faster than GeoPandas</li>
                    <li class="fragment"><em>Versatility</em>: Read, write and processing vector, raster all in one.
                    </li>
                    <li class="fragment"><em>Scalability</em>: Can process terabytes of spatial data distributed across
                        a cluster.
                    </li>
                    <li class="fragment"><em>Interoperability</em>: Python, Java, Scala, R and SQL</li>
                    <li class="fragment"><em>Rich Spatial Functions</em>: OGC-compliant functions such as ST_Contains,
                        ST_Distance, ST_Intersects, etc.
                    </li>
                    <li class="fragment"><em>Spatial indexing</em>: Built-in R-tree and Quad-tree indexing to speed up
                        spatial queries
                    </li>
                </ul>

            </section>
                <!--Slide 5-2: Sedona disadvantages-->
            <section id="sedona-disadvantages">
                <h3>Some disadvantages of Sedona</h3>
                <ul>
                    <li class="fragment"><em>Memory-Intensive</em>: Spatial joins and indexing can cause high memory
                        usage on large datasets.
                    </li>
                    <li class="fragment"><em>Deep Learning Curve</em>: Requires knowledge with both Spark and geospatial
                        concepts.
                    </li>
                    <li class="fragment"><em>Limited Raster Support</em>: Raster support is basic compared to
                        specialized GIS tools.
                    </li>
                    <li class="fragment"><em>Limit cartography Support</em>: Lacks advanced cartography and
                        visualization features
                    </li>

                </ul>
            </section>

        </section>

        <!--Slide 6: Key concepts in sedona-->
        <section data-transition="fade-in slide-out">
            <!--Slide 6-1: sedona general architecture-->
            <section>
                <h3>Key Concepts in sedona</h3>
                <img src="assets/apache-sedona_ecosys.webp" alt="apache-sedona_ecosys.webp"/>
            </section>
            <!--Slide 6-2: key concept in sedona-->
            <section>
                <h3>Kep concepts in sedona(spark):</h3>

                <ul>
                    <li class="fragment"><em>Spatial RDDs </em>: Core data model in sedona(spark) for storing spatial
                        data
                    </li>
                    <li class="fragment"><em>Spatial DataFrame</em>: An abstraction of Spatial RDDs to simplify usage
                    </li>
                    <li class="fragment"><em>Spatial Functions</em>: Predefined spatial functions
                    </li>
                    <li class="fragment"><em>Spatial Partitioning</em>: Distributes data across nodes using spatial
                        boundaries to optimize joins.
                    </li>
                    <li class="fragment"><em>Spatial Indexing</em>: Indexes geometries (R-tree, Quad-tree) columns to
                        accelerate queries calculation.
                    </li>
                </ul>
            </section>
             <!--Slide 6-3: spatial rdd in sedona-->
            <section>
                <h3>Spatial RDDs in Sedona(Spark):</h3>
                <p>Sedona adds <em>spatial metadata, spatial indexing</em> on top of Spark’s standard RDD to build a
                    Spatial RDD.</p>
                <ul>
                    <li class="fragment"><em>PointRDD </em>: Core data model to represent points
                    </li>
                    <li class="fragment"><em>LineStringRDD</em>: Core data model to represent lines
                    </li>
                    <li class="fragment"><em>PolygonRDD</em>: Core data model to represent polygons
                    </li>
                    <li class="fragment"><em>RectangleRDD</em>: Core data model to represent bounding boxes
                    </li>
                    <li class="fragment"><em>RasterRDD</em>: Core data model to represent raster object
                    </li>
                </ul>
            </section>
             <!--Slide 6-3: spatial dataframe in sedona-->
            <section>
                <h3>Spatial Dataframe in Sedona(Spark):</h3>
                <p>Sedona adds <em>geometry, raster</em> user define column type on top of Spark’s dataframe to build a
                    Spatial Dataframe. For all spatial columns, it adds spatial SQL function supports(e.g. ST_Contains,
                    ST_Transform, etc.)</p>
                <ul>
                    <li class="fragment"><em>geometry</em>: stores point, line, polygon, list of polygons
                    </li>
                    <li class="fragment"><em>raster</em>: stores pixel grids, CRS, etc.
                    </li>
                    <li class="fragment"><em>geometry sql operators</em>: Thirty plus pre-defined functions for geometry columns
                    </li>
                    <li class="fragment"><em>raster sql operators</em>: Twenty plus pre-defined functions for geometry columns
                </ul>
            </section>
        </section>

        <!--Slide 7: sedona in CASD-->
        <section data-transition="fade-in slide-out">
            <!-- Slide 7-1: overview      -->
            <section>
                <h2>Sedona in CASD</h2>

                <ul>
                    <li class="fragment"><em>Computing engine</em>: spark</li>
                    <li class="fragment"><em>Spark client API</em>: Python, Java, Scala, SQL, R</li>
                    <li class="fragment"><em>Integrated Development Environment (IDE)</em>: vs-code, r-studio, jupyter
                        notebook
                    </li>
                    <li class="fragment"><em>Project Structure & Configuration Files</em>: CASD best practices</li>
                </ul>
            </section>
            <!-- Slide 7-2: Install sedona in casd      -->
            <section>
                <h3>Install sedona in CASD</h3>
                <pre data-id="code-animation"><code class="language-Bash" data-trim
                                                    data-line-numbers="|11-14|"><script
                        type="text/template">
# step1: Install the latest sedona framework
# CASD provides multiple versions of sedona. But by default, CASD only offers the jar files of the
# `latest version of sedona`. If you want to use a specific version of sedona, you need to contact `service@casd.eu`
cd C:\Users\Public\Documents\sedona\sedona-35-212-172

# create a folder called jars in your project, and copy the folder `sedona-35-212-172` into jars
cp -r .\sedona-35-212-172 path\to\your-project\jars


# step2: Install the apache-sedona python support
# install sedona with spark extension
pip install apache-sedona[spark]
# show the installed package version
pip show apache-sedona

					</script></code></pre>

            </section>
        </section>

    </div>
</div>

<script src="dist/reveal.js"></script>
<script src="plugin/notes/notes.js"></script>
<script src="plugin/markdown/markdown.js"></script>
<script src="plugin/highlight/highlight.js"></script>
<script>
    // More info about initialization & config:
    // - https://revealjs.com/initialization/
    // - https://revealjs.com/config/
    Reveal.initialize({
        hash: true,

        // Learn about plugins: https://revealjs.com/plugins/
        plugins: [RevealMarkdown, RevealHighlight, RevealNotes],
    });
</script>
</body>

</html>